## ðŸŸ¦ v5.10.0 Patch Plan (Data Foundations)

**Objective:** Add *read-only* execution intelligence:

1. **Time-of-day expectancy & slippage maps** from router/trade logs.
2. **Symbol scoring model** combining Sharpe, ATR regime, router KPIs, fees, DD.

No behavior changes yet â€” just analytics helpers + tests.

### Files to add/modify

**New package:**

* `execution/intel/__init__.py`
* `execution/intel/expectancy_map.py`
* `execution/intel/symbol_score.py`

**New tests:**

* `tests/test_expectancy_map.py`
* `tests/test_symbol_score.py`

---

## âœ… Commit 1 â€” Add execution.intel package & expectancy_map

**Commit message:**

```text
[v5.10.0] Add execution.intel package and hourly expectancy map skeleton
```

### `execution/intel/__init__.py`

```python
"""
Execution Intelligence (v5.10+).

This package holds higher-level analytics and policies that sit on top of the
core execution and telemetry stack. All logic here should be:

- Read-only or pure functions where possible.
- Driven by existing logs and metrics (router_metrics, fills, PnL, etc.).
- Covered by focused pytest modules in tests/test_*intel*.py.
"""
```

---

### `execution/intel/expectancy_map.py` (skeleton)

```python
from __future__ import annotations

"""
Time-of-day expectancy and slippage analytics.

This module reads router/trade telemetry and buckets it by hour-of-day so we
can see when a symbol tends to trade well (positive expectancy, low slippage)
or poorly (negative expectancy, high slippage).

v5.10.0 adds a minimal, read-only skeleton:
- No behavior changes.
- API designed for later integration with dashboard and router tuning.
"""

from dataclasses import dataclass
from typing import Dict, List, Optional, Iterable
import datetime as dt

from execution.router_metrics import get_recent_router_events


@dataclass
class HourlyStats:
    """Aggregate stats for a single hour-of-day bucket."""
    count: int = 0
    pnl_sum: float = 0.0          # aggregate realized PnL (if available)
    notional_sum: float = 0.0     # aggregate notional traded
    slip_bps_sum: float = 0.0     # sum of slippage in bps
    slip_bps_count: int = 0       # count of events with slippage


def _to_hour(ts: int) -> int:
    """Convert epoch seconds to 0â€“23 hour-of-day (UTC for now)."""
    return dt.datetime.utcfromtimestamp(ts).hour


def _iter_events(symbol: Optional[str] = None) -> Iterable[dict]:
    """
    Thin wrapper around router_metrics.get_recent_router_events.

    For v5.10.0, we default to a 7d window and leave symbol=None as
    aggregate mode.
    """
    # NOTE: .get_recent_router_events is already used by router_effectiveness_7d
    return get_recent_router_events(symbol=symbol, window_days=7)


def _update_stats(stats: HourlyStats, event: dict) -> None:
    """
    Update a single HourlyStats bucket with one router/trade event.

    v5.10.0 skeleton:
    - Treat `pnl` and `notional` as optional fields.
    - Treat `slippage_bps` as optional.
    """
    stats.count += 1

    pnl = event.get("realized_pnl", 0.0) or 0.0
    notional = event.get("notional", 0.0) or 0.0
    slip = event.get("slippage_bps")

    stats.pnl_sum += float(pnl)
    stats.notional_sum += float(notional)

    if slip is not None:
        stats.slip_bps_sum += float(slip)
        stats.slip_bps_count += 1


def hourly_expectancy(symbol: Optional[str] = None) -> Dict[int, dict]:
    """
    Compute simple per-hour aggregates for the last N days.

    Returns:
        {hour: {"count": int,
                "exp_per_notional": float | None,
                "slip_bps_avg": float | None}}

    - exp_per_notional is pnl_sum / notional_sum when notional_sum > 0.
    - slip_bps_avg is average slippage in bps when slippage samples exist.
    """
    buckets: Dict[int, HourlyStats] = {}

    for ev in _iter_events(symbol=symbol):
        ts = ev.get("ts")
        if ts is None:
            continue
        hour = _to_hour(int(ts))

        stats = buckets.get(hour)
        if stats is None:
            stats = HourlyStats()
            buckets[hour] = stats

        _update_stats(stats, ev)

    result: Dict[int, dict] = {}
    for hour, s in buckets.items():
        exp_per_notional: Optional[float]
        if s.notional_sum > 0:
            exp_per_notional = s.pnl_sum / s.notional_sum
        else:
            exp_per_notional = None

        slip_avg: Optional[float]
        if s.slip_bps_count > 0:
            slip_avg = s.slip_bps_sum / s.slip_bps_count
        else:
            slip_avg = None

        result[hour] = {
            "count": s.count,
            "exp_per_notional": exp_per_notional,
            "slip_bps_avg": slip_avg,
        }

    return result


def best_hours(
    symbol: str,
    min_trades: int = 20,
    min_expectancy: float = 0.0,
) -> List[int]:
    """
    Return hours-of-day where expectancy looks favorable.

    A "best" hour is any bucket where:
      - trade count >= min_trades
      - exp_per_notional >= min_expectancy

    v5.10.0 keeps the policy intentionally simple; future versions can add
    slippage filters, volatility filters, etc.
    """
    stats = hourly_expectancy(symbol)
    good: List[int] = []
    for hour, row in stats.items():
        if row["count"] < min_trades:
            continue
        exp_val = row["exp_per_notional"]
        if exp_val is None:
            continue
        if exp_val >= min_expectancy:
            good.append(hour)
    return sorted(good)
```

---

### `tests/test_expectancy_map.py` (skeleton)

```python
# tests/test_expectancy_map.py

from execution.intel.expectancy_map import hourly_expectancy, best_hours


def test_hourly_expectancy_buckets_events_execution_intelligence(monkeypatch):
    # Fake router events across two hours
    fake_events = [
        {"symbol": "BTCUSDC", "ts": 1707345600, "realized_pnl": 10.0, "notional": 1000.0, "slippage_bps": 1.0},
        {"symbol": "BTCUSDC", "ts": 1707345600, "realized_pnl": -5.0, "notional": 500.0, "slippage_bps": 2.0},
        {"symbol": "BTCUSDC", "ts": 1707349200, "realized_pnl": 2.0, "notional": 200.0, "slippage_bps": 0.5},
    ]

    def fake_get_recent_router_events(symbol=None, window_days=7):
        return fake_events

    monkeypatch.setattr(
        "execution.intel.expectancy_map.get_recent_router_events",
        fake_get_recent_router_events,
    )

    stats = hourly_expectancy("BTCUSDC")
    assert isinstance(stats, dict)
    assert sum(row["count"] for row in stats.values()) == 3


def test_best_hours_filters_by_trades_and_expectancy_execution_intelligence(monkeypatch):
    # Force hourly_expectancy to return deterministic buckets
    def fake_hourly_expectancy(symbol=None):
        return {
            9: {"count": 10, "exp_per_notional": -0.001, "slip_bps_avg": 2.0},
            10: {"count": 25, "exp_per_notional": 0.002, "slip_bps_avg": 1.0},
        }

    monkeypatch.setattr(
        "execution.intel.expectancy_map.hourly_expectancy",
        fake_hourly_expectancy,
    )

    hours = best_hours("BTCUSDC", min_trades=20, min_expectancy=0.0)
    assert hours == [10]
```

---

## âœ… Commit 2 â€” Add symbol scoring model

**Commit message:**

```text
[v5.10.0] Add symbol scoring skeleton combining Sharpe, ATR, router KPIs, and DD
```

### `execution/intel/symbol_score.py` (skeleton)

```python
from __future__ import annotations

"""
Symbol scoring model for execution intelligence (v5.10.0).

This module combines existing metrics (Sharpe, ATR regime, router effectiveness,
fees, DD) into a single "quality score" per symbol.

The score is intended to:
- Drive size multipliers in later v5.10.x commits.
- Support dashboards and routing policies.
"""

from dataclasses import dataclass
from typing import Dict, Any, Optional

from execution.utils.metrics import (
    rolling_sharpe_7d,
    router_effectiveness_7d,
    dd_today_pct,
)
from execution.utils.vol import atr_pct


@dataclass
class SymbolScoreComponents:
    sharpe: float = 0.0
    sharpe_score: float = 0.0
    atr_ratio: Optional[float] = None
    atr_score: float = 0.0
    router_score: float = 0.0
    dd_pct: float = 0.0
    dd_score: float = 0.0
    total: float = 0.0


def _score_sharpe(sharpe: Optional[float]) -> float:
    """
    Map Sharpe to a bounded contribution.

    Rough skeleton:
    - Sharpe <= -1 â†’ -1
    - Sharpe  ~ 0 â†’ 0
    - Sharpe >=  2 â†’ +1
    """
    if sharpe is None:
        return 0.0
    if sharpe <= -1.0:
        return -1.0
    if sharpe >= 2.0:
        return 1.0
    # linear interpolation between -1 and 2
    return (sharpe / 2.0)


def _score_atr_ratio(atr_now: float, atr_med: float) -> float:
    """
    Score the ATR ratio (short/long volatility).

    Idea:
    - very quiet / very extreme regimes are penalized slightly
    - normal volatility is neutral
    """
    if atr_med <= 0:
        return 0.0
    ratio = atr_now / atr_med
    if ratio < 0.5:
        return -0.2
    if ratio > 2.5:
        return -0.3
    if 0.8 <= ratio <= 1.5:
        return 0.1
    return 0.0


def _score_router(eff: Dict[str, Any]) -> float:
    """
    Score router effectiveness.

    Skeleton:
    - High maker fill ratio and low fallback ratio contribute positively.
    - Very high fallback or bad median slippage contribute negatively.
    """
    maker = eff.get("maker_fill_ratio") or 0.0
    fallback = eff.get("fallback_ratio") or 0.0
    slip_med = eff.get("slip_q50")

    score = 0.0
    score += (maker - 0.5) * 0.5       # gently reward >50% maker fills
    score -= max(0.0, fallback - 0.5)  # penalize fallback above 50%

    if slip_med is not None and slip_med > 5.0:
        score -= 0.3
    return score


def _score_dd(dd_pct: float) -> float:
    """
    Score today's DD (drawdown).

    Skeleton:
    - dd <= -3% â†’ -0.5
    - dd ~ 0%  â†’ 0
    - dd > 0   â†’ small positive (if we ever encode that)
    """
    if dd_pct <= -3.0:
        return -0.5
    if dd_pct <= -1.5:
        return -0.25
    if dd_pct >= 0.5:
        return 0.1
    return 0.0


def compute_symbol_score(symbol: str) -> Dict[str, Any]:
    """
    Compute a symbol quality score.

    Returns:
        {
          "symbol": str,
          "score": float,
          "components": {
            "sharpe": float,
            "sharpe_score": float,
            "atr_ratio": float | None,
            "atr_score": float,
            "router_score": float,
            "dd_pct": float,
            "dd_score": float,
          },
        }

    v5.10.0 intentionally uses a simple linear scheme; future sprints may
    refine the weighting or add more inputs.
    """
    sharpe = rolling_sharpe_7d(symbol)
    sharpe_contrib = _score_sharpe(sharpe)

    # ATR regime via ratio of short vs long ATR%
    atr_now = atr_pct(symbol, lookback_bars=50)
    atr_med = atr_pct(symbol, lookback_bars=500)
    atr_contrib = _score_atr_ratio(atr_now, atr_med) if atr_med > 0 else 0.0
    atr_ratio = (atr_now / atr_med) if atr_med > 0 else None

    router_eff = router_effectiveness_7d(symbol)
    router_contrib = _score_router(router_eff)

    dd = dd_today_pct(symbol) or 0.0
    dd_contrib = _score_dd(dd)

    total = sharpe_contrib + atr_contrib + router_contrib + dd_contrib

    comps = SymbolScoreComponents(
        sharpe=sharpe or 0.0,
        sharpe_score=sharpe_contrib,
        atr_ratio=atr_ratio,
        atr_score=atr_contrib,
        router_score=router_contrib,
        dd_pct=dd,
        dd_score=dd_contrib,
        total=total,
    )

    return {
        "symbol": symbol,
        "score": comps.total,
        "components": {
            "sharpe": comps.sharpe,
            "sharpe_score": comps.sharpe_score,
            "atr_ratio": comps.atr_ratio,
            "atr_score": comps.atr_score,
            "router_score": comps.router_score,
            "dd_pct": comps.dd_pct,
            "dd_score": comps.dd_score,
        },
    }


def score_to_size_factor(score: float) -> float:
    """
    Map a symbol score into a size multiplier.

    Skeleton:
    - score <= -1.0 â†’ 0.5x size
    - score ~ 0    â†’ 1.0x size
    - score >= +1.5 â†’ 1.5x size
    - Always clamp to [0.25, 2.0]
    """
    if score <= -1.0:
        factor = 0.5
    elif score >= 1.5:
        factor = 1.5
    else:
        # interpolate roughly around zero
        factor = 1.0 + 0.3 * score

    return max(0.25, min(2.0, factor))
```

---

### `tests/test_symbol_score.py` (skeleton)

```python
# tests/test_symbol_score.py

from execution.intel.symbol_score import compute_symbol_score, score_to_size_factor


def test_compute_symbol_score_uses_metrics_execution_intelligence(monkeypatch):
    # Provide deterministic metric values
    monkeypatch.setattr(
        "execution.intel.symbol_score.rolling_sharpe_7d",
        lambda s: 2.0,
    )
    monkeypatch.setattr(
        "execution.intel.symbol_score.atr_pct",
        lambda s, lookback_bars=50: 1.0 if lookback_bars == 50 else 1.0,
    )
    monkeypatch.setattr(
        "execution.intel.symbol_score.router_effectiveness_7d",
        lambda s: {"maker_fill_ratio": 0.7, "fallback_ratio": 0.2, "slip_q50": 2.0},
    )
    monkeypatch.setattr(
        "execution.intel.symbol_score.dd_today_pct",
        lambda s: -0.5,
    )

    result = compute_symbol_score("BTCUSDC")
    assert result["symbol"] == "BTCUSDC"
    assert isinstance(result["score"], float)
    comps = result["components"]
    assert comps["sharpe"] == 2.0
    assert "router_score" in comps
    assert "atr_ratio" in comps


def test_score_to_size_factor_monotonic_execution_intelligence():
    low = score_to_size_factor(-2.0)
    mid = score_to_size_factor(0.0)
    high = score_to_size_factor(2.0)

    assert low < mid < high
    assert 0.25 <= low <= 1.0
    assert mid == 1.0
    assert high <= 2.0
```

---

## ðŸ” After applying v5.10.0

From venv:

```bash
pytest tests/test_expectancy_map.py tests/test_symbol_score.py -q
```