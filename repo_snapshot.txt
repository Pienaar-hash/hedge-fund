## dashboard/__init__.py

## dashboard/app.py
#!/usr/bin/env python3
# ruff: noqa: E402
from __future__ import annotations
"""Streamlit dashboard (single "Overview" tab), read-only.
Firestore-first; falls back to local files for NAV.
Shows: KPIs, NAV chart, Positions, Signals (5), Trade log (5), Screener tail (10), BTC reserve.
"""
# Streamlit dashboard (single "Overview" tab), read-only.
# Firestore-first; falls back to local files for NAV.

# ---- tolerant dotenv ----
try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass

import os
import json
import time
from typing import Any, Dict, List, Optional
import pandas as pd
import streamlit as st
from pathlib import Path

from execution import nav as navmod
from dashboard.nav_helpers import (
    format_treasury_table,
    signal_attempts_summary,
    treasury_table_from_summary,
)

# Diagnostics container populated during data loads
_DIAG: Dict[str, Any] = {
    "source": None,
    "fs_project": None,
    "col_nav": None,
    "col_positions": None,
    "col_trades": None,
    "col_risk": None,
}

# ---- single env context ----
ENV = os.getenv("ENV", "prod")
TESTNET = os.getenv("BINANCE_TESTNET", "0") == "1"
ENV_KEY = f"{ENV}{'-testnet' if TESTNET else ''}"

RESERVE_BTC = float(os.getenv("RESERVE_BTC", "0.025"))
# Local-only log files when Firestore is not selected
LOG_PATH = os.getenv("EXECUTOR_LOG", f"logs/{ENV_KEY}/screener_tail.log")
TAIL_LINES = 10
RISK_CFG_PATH = os.getenv("RISK_LIMITS_CONFIG", "config/risk_limits.json")

# ---------- helpers ----------
def btc_24h_change() -> float | None:
    import requests
    # Use TESTNET flag from ENV wiring; default to mainnet
    base = "https://testnet.binancefuture.com" if TESTNET else "https://fapi.binance.com"
    try:
        r = requests.get(base + "/fapi/v1/ticker/24hr", params={"symbol":"BTCUSDT"}, timeout=6)
        r.raise_for_status()
        return float(r.json().get("priceChangePercent", 0.0))
    except Exception:
        try:
            r = requests.get("https://api.binance.com/api/v3/ticker/24hr", params={"symbol":"BTCUSDT"}, timeout=6)
            r.raise_for_status()
            return float(r.json().get("priceChangePercent", 0.0))
        except Exception:
            return None

def _load_json(path: str, default=None):
    try:
        with open(path, "r") as f:
            return json.load(f)
    except Exception:
        return default

# Firestore (best effort)
def _fs_client():
    """Return Firestore client if libs + credentials are available.
    Firestore is authoritative only if:
    - google.cloud.firestore imports OK
    - And credentials file path is present: FIREBASE_CREDS_PATH or GOOGLE_APPLICATION_CREDENTIALS
    """
    try:
        from google.cloud import firestore  # type: ignore
    except Exception:
        return None

    creds_path = os.getenv("FIREBASE_CREDS_PATH") or os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
    if creds_path and Path(creds_path).exists():
        try:
            return firestore.Client()
        except Exception:
            return None
    return None

# --- Firestore paths helper (correct sibling collections) ---
def fs_paths(db):
    ROOT_DOC = db.collection("hedge").document(ENV)
    STATE_COL = ROOT_DOC.collection("state")
    return {
        "root_doc": ROOT_DOC,
        "state_col": STATE_COL,
        "nav_doc": STATE_COL.document("nav"),
        "positions_doc": STATE_COL.document("positions"),
        "trades_col": ROOT_DOC.collection("trades"),
        "risk_col": ROOT_DOC.collection("risk"),
    }

def _fs_get_state(doc: str):
    """Back-compat read for single-doc state if present under old path.
    This is best-effort and namespacing by ENV only. Prefer collection helpers below.
    """
    cli = _fs_client()
    if not cli:
        return None
    try:
        snap = cli.document(f"hedge/{ENV}/state/{doc}").get()
        return snap.to_dict() if getattr(snap, "exists", False) else None
    except Exception:
        return None

# ---- Firestore helpers (authoritative when selected) ----
def _fs_pick_collection(cli, candidates: List[str]) -> Optional[str]:
    """Pick the first collection that appears to have docs. Returns name or None.
    Document choice in Doctor panel later.
    """
    for name in candidates:
        try:
            it = cli.collection(name).limit(1).stream()
            for _ in it:
                return name
        except Exception:
            continue
    return None

def _filter_env_fields(doc: Dict[str, Any]) -> bool:
    """Client-side filter: if env/testnet fields exist, they must match.
    Always applied for generic collections.
    """
    d_env = doc.get("env")
    d_tn = doc.get("testnet")
    if d_env is not None and str(d_env) != ENV:
        return False
    if d_tn is not None and bool(d_tn) != TESTNET:
        return False
    return True

def _is_mixed_namespaces(docs: List[Dict[str, Any]]) -> bool:

## dashboard/dashboard_utils.py
import os
import json
from typing import Any, Dict, List, Tuple

import pandas as pd

# Optional Streamlit caching if running under Streamlit; safe no-op otherwise
try:
    import streamlit as st
    _HAVE_ST = True
except Exception:  # pragma: no cover
    _HAVE_ST = False
    class _Dummy:
        def cache_resource(self, **kwargs):
            def deco(fn):
                return fn
            return deco
    st = _Dummy()

# ---------------------------- Firestore --------------------------------------
_DB = None

@st.cache_resource(show_spinner=False)
def get_firestore_connection():
    """Return a cached Firestore client using FIREBASE_CREDS_PATH if provided.

    Expects GOOGLE Cloud Firestore library to be available. If credentials
    are not provided, it will try Application Default Credentials.
    """
    global _DB
    if _DB is not None:
        return _DB
    try:
        from google.cloud import firestore  # type: ignore
    except Exception as e:  # pragma: no cover
        raise RuntimeError("google-cloud-firestore not installed: pip install google-cloud-firestore") from e

    creds_path = os.getenv("FIREBASE_CREDS_PATH")
    if creds_path and os.path.exists(creds_path):
        os.environ.setdefault("GOOGLE_APPLICATION_CREDENTIALS", creds_path)
    _DB = firestore.Client()
    return _DB


def _doc_path(env: str, name: str) -> Tuple[str, str, str, str]:
    """Canonical path used by the project: hedge/{env}/state/{name}."""
    return ("hedge", env, "state", name)


def fetch_state_document(name: str, env: str = "prod") -> Dict[str, Any]:
    """Load a state document from Firestore. Returns {} if not found.

    Path: hedge/{env}/state/{name}
    """
    db = get_firestore_connection()
    c1, d1, c2, d2 = _doc_path(env, name)
    snap = (
        db.collection(c1).document(d1).collection(c2).document(d2).get()
    )
    return snap.to_dict() or {}

# ---------------------------- NAV helpers ------------------------------------

def _points_from_nav_doc(nav_doc: Dict[str, Any]) -> List[Tuple[int, float]]:
    """Best-effort extraction of [(ts, equity)] pairs from various shapes."""
    # Supported keys: points, nav, equity_curve, series
    candidates = (
        nav_doc.get("points")
        or nav_doc.get("nav")
        or nav_doc.get("equity_curve")
        or nav_doc.get("series")
        or []
    )
    pts: List[Tuple[int, float]] = []
    if isinstance(candidates, list):
        for x in candidates:
            if isinstance(x, dict):
                ts = x.get("ts") or x.get("t") or x.get("time")
                eq = x.get("equity") or x.get("v") or x.get("value")
            elif isinstance(x, (list, tuple)) and len(x) >= 2:
                ts, eq = x[0], x[1]
            else:
                continue
            try:
                ts_i = int(float(ts))
                eq_f = float(eq)
                pts.append((ts_i, eq_f))
            except Exception:
                continue
    return pts


def parse_nav_to_df_and_kpis(nav_doc: Dict[str, Any]) -> Tuple[pd.DataFrame, Dict[str, float]]:
    """Return (DataFrame, KPIs) where DF has an `equity` column indexed by datetime.

    KPIs has keys: total_equity, peak_equity, drawdown, unrealized_pnl, realized_pnl
    """
    pts = _points_from_nav_doc(nav_doc)
    if not pts:
        df = pd.DataFrame(columns=["equity"])  # empty
        kpis = dict(
            total_equity=float(nav_doc.get("total_equity", 0.0)),
            peak_equity=float(nav_doc.get("peak_equity", 0.0)),
            drawdown=float(nav_doc.get("drawdown", 0.0)),
            unrealized_pnl=float(nav_doc.get("unrealized_pnl", 0.0)),
            realized_pnl=float(nav_doc.get("realized_pnl", 0.0)),
        )
        return df, kpis

    # Normalize to DataFrame
    ts = [t/1000 if t > 10_000_000_000 else t for t, _ in pts]  # ms -> s if needed
    eq = [v for _, v in pts]
    idx = pd.to_datetime(ts, unit="s", utc=True)
    df = pd.DataFrame({"equity": eq}, index=idx).sort_index()

    total_equity = float(eq[-1])
    peak_equity = float(max(eq)) if eq else total_equity
    drawdown = 0.0 if peak_equity <= 0 else max(0.0, (peak_equity - total_equity) / peak_equity)

    kpis = dict(
        total_equity=total_equity,
        peak_equity=float(nav_doc.get("peak_equity", peak_equity)),
        drawdown=float(nav_doc.get("drawdown", drawdown)),
        unrealized_pnl=float(nav_doc.get("unrealized_pnl", 0.0)),
        realized_pnl=float(nav_doc.get("realized_pnl", 0.0)),
    )
    return df, kpis

# ---------------------------- Misc helpers -----------------------------------

def positions_sorted(items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Sort open positions by descending notional (abs)."""
    def _abs_notional(x: Dict[str, Any]) -> float:
        try:
            return abs(float(x.get("notional", 0.0)))
        except Exception:
            return 0.0
    return sorted(items or [], key=_abs_notional, reverse=True)


def read_trade_log_tail(path: str, tail: int = 10) -> List[Dict[str, Any]]:
    """Read last N trades from either JSON array or JSON Lines file.
    Returns empty list if file missing or malformed.
    """
    try:
        with open(path, "r") as f:
            txt = f.read().strip()
    except Exception:
        return []


## dashboard/firestore_helpers.py
from utils.firestore_client import get_db
import os

def _doc(name:str):
    env = os.environ.get("ENV","dev")
    db = get_db()
    return db.collection("hedge").document(env).collection("state").document(name).get()

def read_leaderboard(env=None):
    return _doc("leaderboard").to_dict() or {}

def read_nav(env=None):
    return _doc("nav").to_dict() or {}

def read_positions(env=None):
    return _doc("positions").to_dict() or {}

## dashboard/main.py
import os
import sys
import json
import time
import subprocess
from typing import List

import pandas as pd
import streamlit as st

# Ensure the project root is importable
PROJECT_ROOT = "/root/hedge-fund"
if PROJECT_ROOT not in sys.path and os.path.isdir(PROJECT_ROOT):
    sys.path.insert(0, PROJECT_ROOT)

# Local helpers
from dashboard.dashboard_utils import (
    get_firestore_connection,
    fetch_state_document,
    parse_nav_to_df_and_kpis,
    positions_sorted,
    fetch_mark_price_usdt,
    get_env_float,
)

# Read-only exchange helpers

# Doctor helper
# removed: doctor runs via subprocess now

# --------------------------- Page config -------------------------------------
st.set_page_config(page_title="Hedge â€” Portfolio Dashboard", layout="wide")
ENV = os.getenv("ENV", "prod")
REFRESH_SEC = int(os.getenv("DASHBOARD_REFRESH_SEC", "60"))
TRADE_LOG = os.getenv("TRADE_LOG", "trade_log.json")

# log-tail behavior
TAIL_BYTES = int(os.getenv("DASHBOARD_LOG_TAIL_BYTES", "200000"))
TAIL_LINES = int(os.getenv("DASHBOARD_SIGNAL_LINES", "80"))
WANT_TAGS = tuple((os.getenv("DASHBOARD_SIGNAL_TAGS") or "[screener],[screener->executor],[decision]").split(","))

# stable-coins to include in equity calc
STABLES = [s.strip() for s in (os.getenv("DASHBOARD_STABLES", "USDT,FDUSD,BFUSD,USDC").split(",")) if s.strip()]

# Optional auto-refresh
try:
    from streamlit_extras.st_autorefresh import st_autorefresh
    st_autorefresh(interval=REFRESH_SEC * 1000, key="auto_refresh")
except Exception:
    pass  # manual refresh only

st.title("ðŸ“Š Hedge â€” Portfolio Dashboard")
st.caption(f"ENV = {ENV} Â· refresh â‰ˆ {REFRESH_SEC}s")

# --------------------------- Small utilities ---------------------------------
def load_json(path: str, default=None):
    try:
        with open(path, "r") as f:
            return json.load(f)
    except Exception:
        return {} if default is None else default

def human_age(ts) -> str:
    if not ts:
        return "â€“"
    try:
        now = int(time.time())
        d = now - int(ts)
        if d < 60:
            return f"{d}s"
        if d < 3600:
            return f"{d//60}m"
        return f"{d//3600}h"
    except Exception:
        return "â€“"

def tail_text(path: str, max_bytes: int = TAIL_BYTES) -> str:
    try:
        with open(path, "rb") as f:
            f.seek(0, os.SEEK_END)
            size = f.tell()
            f.seek(max(0, size - max_bytes), os.SEEK_SET)
            return f.read().decode(errors="ignore")
    except Exception as e:
        return f"(cannot read {path}: {e})"

def rle_compact(lines: List[str], min_run: int = 3) -> List[str]:
    if not lines:
        return lines
    out = []
    prev = lines[0]
    count = 1
    for ln in lines[1:]:
        if ln == prev:
            count += 1
        else:
            out.append(prev if count < min_run else f"{prev}  Ã— {count}")
            prev = ln
            count = 1
    out.append(prev if count < min_run else f"{prev}  Ã— {count}")
    return out

# --------------------------- Load Firestore ----------------------------------
status = st.empty()
status.info("Loading data from Firestoreâ€¦")

try:
    db = get_firestore_connection()
    nav_doc = fetch_state_document("nav", env=ENV)
    pos_doc = fetch_state_document("positions", env=ENV)
    lb_doc  = fetch_state_document("leaderboard", env=ENV)
except Exception as e:
    st.error(f"Firestore read failed: {e}")
    st.stop()

nav_df, kpis = parse_nav_to_df_and_kpis(nav_doc)
positions_fs = positions_sorted(pos_doc.get("items") or [])
leaderboard = lb_doc.get("items") or []

status.success(f"Loaded Â· updated_at={nav_doc.get('updated_at','â€”')}")

# ---- Read-only Reserve KPI ---------------------------------------------------
RESERVE_BTC = get_env_float("DASHBOARD_RESERVE_BTC", 0.13)
btc_mark = fetch_mark_price_usdt("BTCUSDT")
reserve_usdt = RESERVE_BTC * btc_mark if btc_mark > 0 else 0.0

# ---- Tabs layout --------------------------------------------------------------
tab_overview, tab_positions, tab_leader, tab_signals, tab_ml, tab_doctor = st.tabs(
    ["Overview", "Positions", "Leaderboard", "Signals", "ML", "Doctor"]
)

# --------------------------- Overview Tab ------------------------------------
with tab_overview:
    st.subheader("Portfolio KPIs")
    equity = kpis.get("total_equity")
    peak_equity = kpis.get("peak_equity")
    drawdown_pct = kpis.get("drawdown")
    realized_pnl = kpis.get("realized_pnl")
    unrealized_pnl = kpis.get("unrealized_pnl")

    k1, k2, k3, k4, k5, k6 = st.columns(6)
    k1.metric("Equity (USDT)", f"{equity:,.0f}" if equity is not None else "â€”")
    k2.metric("Peak (USDT)", f"{peak_equity:,.0f}" if peak_equity is not None else "â€”")
    k3.metric("Drawdown (%)", f"{drawdown_pct:.2f}%" if drawdown_pct is not None else "â€”")
    k4.metric("Realized PnL", f"{realized_pnl:,.0f}" if realized_pnl is not None else "â€”")
    k5.metric("Unrealized PnL", f"{unrealized_pnl:,.0f}" if unrealized_pnl is not None else "â€”")

    reserve_caption = f"~{reserve_usdt:,.0f} USDT" if reserve_usdt > 0 else "â€”"
    k6.metric("Reserve (BTC)", f"{RESERVE_BTC:.3f} BTC", reserve_caption)


## dashboard/nav_helpers.py
"""Helpers for dashboard NAV and screener metrics."""
from __future__ import annotations

from typing import Any, Dict, List

import pandas as pd
from pandas.io.formats.style import Styler

_UNITS_FMT = "{:.6f}"
_USD_FMT = "{:,.2f}"


def treasury_table_from_summary(summary: Dict[str, Any]) -> pd.DataFrame:
    """Return a DataFrame of treasury assets from compute_nav_summary output.

    Expected summary shape:
      {
        "details": {
            "treasury": {
                "treasury": {
                    "BTC": {"qty": 0.1, "val_usdt": 1000.0, ...},
                    ...
                }
            }
        }
      }
    """

    treasury_detail = (summary.get("details") or {}).get("treasury", {})
    holdings = treasury_detail.get("treasury", {}) if isinstance(treasury_detail, dict) else {}
    rows = []
    for asset, info in holdings.items():
        try:
            qty = float(info.get("qty", 0.0) or 0.0)
            usd = float(info.get("val_usdt", 0.0) or 0.0)
        except Exception:
            continue
        rows.append({"Asset": str(asset), "Units": qty, "USD Value": usd})

    if not rows:
        return pd.DataFrame(columns=["Asset", "Units", "USD Value"])

    df = pd.DataFrame(rows)
    df = df.sort_values(by="USD Value", ascending=False).reset_index(drop=True)
    return df


def format_treasury_table(df: pd.DataFrame) -> Styler:
    """Format treasury DataFrame for display."""

    return df.style.format({"Units": _UNITS_FMT, "USD Value": _USD_FMT})


def signal_attempts_summary(lines: List[str]) -> str:
    """Return latest screener attempted/emitted summary string."""

    for line in reversed(lines):
        if "attempted=" not in line or "emitted=" not in line:
            continue
        attempted = _extract_int(line, "attempted")
        emitted = _extract_int(line, "emitted")
        if attempted is None or emitted is None:
            continue
        pct = (emitted / attempted * 100.0) if attempted else 0.0
        pct_display = f" ({pct:.0f}%)" if attempted else ""
        return f"Signals: {attempted} attempted, {emitted} emitted{pct_display}"
    return "Signals: N/A"


def _extract_int(text: str, key: str) -> int | None:
    needle = f"{key}="
    start = text.find(needle)
    if start == -1:
        return None
    start += len(needle)
    end = start
    while end < len(text) and text[end].isdigit():
        end += 1
    try:
        return int(text[start:end])
    except Exception:
        return None


__all__ = [
    "treasury_table_from_summary",
    "format_treasury_table",
    "signal_attempts_summary",
]

## dashboard/theme.css
/* Minimal, neutral theme for Streamlit */
:root {
  --bg: #0f1115;
  --panel: #171a21;
  --text: #e6e6e6;
  --muted: #9aa0a6;
  --accent: #6aa0ff;
  --good: #35c759;
  --bad: #ff453a;
}

body, .stApp { background: var(--bg) !important; color: var(--text) !important; }
section[data-testid="stSidebar"] { background: var(--panel) !important; }
div[data-testid="stMetricValue"] { color: var(--text) !important; }
div[data-testid="stMetricDelta"] { color: var(--muted) !important; }
.block-container { padding-top: 1.5rem; }
hr { border: 0; height: 1px; background: #222631; margin: 1rem 0; }
.stDataFrame thead tr th { background: #1c212b !important; color: var(--text) !important; }
.stDataFrame td { color: var(--text) !important; }
code, pre { background: #10141a !important; color: #cbd5e1 !important; }
a { color: var(--accent) !important; }

## execution/__init__.py

## execution/exchange_utils.py
#!/usr/bin/env python3
from __future__ import annotations

import hashlib
import hmac
import logging
import math
import os
import sys
import time
from typing import Any, Dict, List, Optional, Tuple
from urllib.parse import urlencode
from decimal import ROUND_DOWN, ROUND_UP, Decimal, getcontext, localcontext

import requests
from execution.utils import load_json

getcontext().prec = 28
# --- robust .env load (works under supervisor too) ---
try:
    from dotenv import load_dotenv  # type: ignore

    load_dotenv() or load_dotenv("/root/hedge-fund/.env")
except Exception:
    pass
# manual fallback in case python-dotenv isn't available at runtime
_envp = "/root/hedge-fund/.env"
if os.path.exists(_envp):
    try:
        with open(_envp, "r") as _f:
            for _ln in _f:
                _ln = _ln.strip()
                if not _ln or _ln.startswith("#") or "=" not in _ln:
                    continue
                k, v = _ln.split("=", 1)
                os.environ.setdefault(k.strip(), v.strip())
    except Exception:
        pass

_LOG = logging.getLogger("exutil")
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s %(levelname)s [exutil] %(message)s"
)

# --- Base URL + one-time environment banner ---------------------------------
def _base_url() -> str:
    """Return the USD-M futures base URL based on BINANCE_TESTNET."""
    return (
        "https://testnet.binancefuture.com"
        if os.environ.get("BINANCE_TESTNET", "0") == "1"
        else "https://fapi.binance.com"
    )

_BANNER_ONCE = False
def _log_env_once() -> None:
    """Log base URL and testnet flag once per process (stderr)."""
    global _BANNER_ONCE
    if _BANNER_ONCE:
        return
    _BANNER_ONCE = True
    testnet = os.environ.get("BINANCE_TESTNET", "0") == "1"
    print(f"[exutil] base={_base_url()} testnet={testnet}", file=sys.stderr)

# Run banner at import time (best-effort; never fail)
try:
    _log_env_once()
except Exception:
    pass

# DRY_RUN guard for signed endpoints (allows full loop without signed calls)
_DRY_RUN = os.environ.get("DRY_RUN", "0") == "1"


def is_testnet() -> bool:
    v = os.getenv("BINANCE_TESTNET", "0")
    return str(v).lower() in ("1", "true", "yes", "y")


_BASE = _base_url()
_KEY = os.getenv("BINANCE_API_KEY", "")
_SEC = os.getenv("BINANCE_API_SECRET", "").encode()
_S = requests.Session()
_S.headers["X-MBX-APIKEY"] = _KEY


def _req(
    method: str,
    path: str,
    *,
    signed: bool = False,
    params: Dict[str, Any] | None = None,
    timeout: float = 8.0,
) -> requests.Response:
    """
    Sign EXACTLY what we send. For signed:
      - Build qs with urlencode (preserving insertion order of dict)
      - Sign qs
      - GET/DELETE: put qs+signature in URL
      - POST/PUT:   send qs+signature as x-www-form-urlencoded body
    """
    method = method.upper()
    url = _BASE + path
    params = {k: v for k, v in (params or {}).items() if v is not None}

    if signed:
        params.setdefault("timestamp", int(time.time() * 1000))
        params.setdefault("recvWindow", int(os.getenv("BINANCE_RECV_WINDOW", "5000")))
        kv = [(str(k), str(v)) for k, v in params.items()]
        qs = urlencode(kv, doseq=True, safe=":/")
        sig = hmac.new(_SEC, qs.encode(), hashlib.sha256).hexdigest()
        if method in ("GET", "DELETE"):
            url = f"{url}?{qs}&signature={sig}"
            data = None
        else:
            data = f"{qs}&signature={sig}"
    else:
        if method in ("GET", "DELETE"):
            if params:
                qs = urlencode(
                    [(str(k), str(v)) for k, v in params.items()], doseq=True, safe=":/"
                )
                url = f"{url}?{qs}"
            data = None
        else:
            data = urlencode(
                [(str(k), str(v)) for k, v in params.items()], doseq=True, safe=":/"
            )

    headers = {"X-MBX-APIKEY": _KEY}
    if method in ("POST", "PUT"):
        headers["Content-Type"] = "application/x-www-form-urlencoded"

    r = _S.request(method, url, data=data, timeout=timeout, headers=headers)
    try:
        r.raise_for_status()
        return r
    except Exception as e:
        detail = ""
        try:
            detail = " :: " + r.text
        except Exception:
            pass
        # Breadcrumb on auth-ish errors
        if r.status_code in (400, 401):
            try:
                code = r.json().get("code")
            except Exception:
                code = "?"
            _LOG.error(
                "[executor] AUTH_ERR code=%s testnet=%s key=%sâ€¦ sec_len=%s url=%s",

## execution/executor_live.py
#!/usr/bin/env python3
# ruff: noqa: E402
from __future__ import annotations

import json
import logging
import os
import time
from typing import Any, Callable, Dict, Iterable, List, Optional

import requests
# Optional .env so Supervisor doesn't need to export everything
try:
    from dotenv import load_dotenv

    load_dotenv() or load_dotenv("/root/hedge-fund/.env")
except Exception:
    pass

LOG = logging.getLogger("exutil")
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s %(levelname)s [exutil] %(message)s"
)

# ---- Exchange utils (binance) ----
from execution.exchange_utils import (
    _is_dual_side,
    build_order_payload,
    get_balances,
    get_positions,
    get_price,
    is_testnet,
    send_order,
)
from execution.risk_limits import RiskState, check_order, RiskGate
from execution.nav import compute_nav_pair, compute_treasury_only, PortfolioSnapshot
from execution.utils import (
    load_json,
    write_nav_snapshots_pair,
    write_treasury_snapshot,
    save_json,
)

# ---- Screener (best effort) ----
generate_signals_from_config: Optional[Callable[[], Iterable[Dict[str, Any]]]] = None
try:
    from execution.signal_screener import generate_signals_from_config as _gen

    generate_signals_from_config = _gen
except Exception:
    generate_signals_from_config = None

# ---- Firestore publisher handle (revisions differ) ----
_PUB: Any
try:
    from execution.state_publish import (
        StatePublisher,
        publish_close_audit,
        publish_intent_audit,
        publish_order_audit,
    )

    _PUB = StatePublisher(interval_s=int(os.getenv("FS_PUB_INTERVAL", "60")))
except Exception:

    class _Publisher:
        def __init__(self, interval_s: int = 60):
            self.interval_s = interval_s

    def publish_intent_audit(intent: Dict[Any, Any]) -> None:
        pass

    def publish_order_audit(symbol: str, event: Dict[Any, Any]) -> None:
        pass

    def publish_close_audit(
        symbol: str, position_side: str, event: Dict[Any, Any]
    ) -> None:
        pass

    _PUB = _Publisher(interval_s=int(os.getenv("FS_PUB_INTERVAL", "60")))

# ---- risk limits config ----
_RISK_CFG_PATH = os.getenv("RISK_LIMITS_CONFIG", "config/risk_limits.json")
_RISK_CFG: Dict[str, Any] = {}
try:
    with open(_RISK_CFG_PATH, "r") as fh:
        _RISK_CFG = json.load(fh) or {}
except Exception as e:
    logging.getLogger("exutil").warning(
        "[risk] config load failed (%s): %s", _RISK_CFG_PATH, e
    )

_RISK_STATE = RiskState()

# Build a minimal RiskGate config adapter from legacy risk_limits.json
def _mk_gate_cfg(raw: Dict[str, Any]) -> Dict[str, Any]:
    g = (raw.get("global") or {}) if isinstance(raw, dict) else {}
    # Legacy configs kept caps at top-level; prefer `global` section when present
    if not g and isinstance(raw, dict):
        for key in ("daily_loss_limit_pct", "max_gross_exposure_pct"):
            if key in raw:
                g[key] = raw[key]
    sizing = {
        "min_notional_usdt": g.get("min_notional_usdt", 5.0),
        # Accept either key for portfolio gross NAV cap
        "max_gross_exposure_pct": (
            g.get("max_gross_exposure_pct")
            or g.get("max_portfolio_gross_nav_pct")
            or g.get("max_gross_nav_pct")
            or 0.0
        ),
        # Fallback sensible default if not provided
        "max_symbol_exposure_pct": g.get("max_symbol_exposure_pct", 50.0),
        "max_trade_nav_pct": g.get("max_trade_nav_pct", 0.0),
    }
    risk = {
        "daily_loss_limit_pct": g.get("daily_loss_limit_pct", 5.0),
        # Derive cooldown (minutes) from error circuit cooldown (seconds) if present
        "cooldown_minutes_after_stop": (
            g.get("cooldown_minutes_after_stop")
            or max(
                0.0,
                float(((g.get("error_circuit") or {}).get("cooldown_sec") or 0))
                / 60.0,
            )
        ),
        "max_trades_per_symbol_per_hour": g.get("max_trades_per_symbol_per_hour", 6),
    }
    return {"sizing": sizing, "risk": risk}

_RISK_GATE = RiskGate(_mk_gate_cfg(_RISK_CFG))
_PORTFOLIO_SNAPSHOT = PortfolioSnapshot(load_json("config/strategy_config.json"))
_RISK_GATE.nav_provider = _PORTFOLIO_SNAPSHOT

# ---- knobs ----
SLEEP = int(os.getenv("LOOP_SLEEP", "60"))
MAX_LOOPS = int(os.getenv("MAX_LOOPS", "0") or 0)
DRY_RUN = os.getenv("DRY_RUN", "1").lower() in ("1", "true", "yes")
INTENT_TEST = os.getenv("INTENT_TEST", "0").lower() in ("1", "true", "yes")

# ------------- helpers -------------


def _account_snapshot() -> None:
    try:
        bals = get_balances() or []
        assets = sorted({b.get("asset", "?") for b in bals})
        pos = [
            p

## execution/firestore_utils.py
# execution/firestore_utils.py
import os

import firebase_admin
from firebase_admin import credentials, firestore

_firestore_client = None


def get_firestore():
    global _firestore_client
    if _firestore_client is None:
        creds_path = os.environ.get("FIREBASE_CREDS_PATH")
        if not creds_path or not os.path.exists(creds_path):
            raise FileNotFoundError(f"Firebase credentials not found at {creds_path}")
        cred = credentials.Certificate(creds_path)
        firebase_admin.initialize_app(cred)
        _firestore_client = firestore.client()
    return _firestore_client


def fetch_leaderboard(limit=10):
    """Fetch leaderboard from Firestore."""
    db = get_firestore()
    docs = (
        db.collection("leaderboard")
        .order_by("pnl", direction=firestore.Query.DESCENDING)
        .limit(limit)
        .stream()
    )
    return [doc.to_dict() for doc in docs]

## execution/flatten_all.py
#!/usr/bin/env python3
"""Flatten all hedge legs safely (MARKET closes with correct positionSide)."""
import argparse
from typing import List


from execution.exchange_utils import get_positions, place_market_order




def flatten(symbols: List[str], confirm: bool = False):
positions = get_positions()
to_close = []
for p in positions:
sym = p.get("symbol")
if symbols and sym not in symbols:
continue
qty = float(p.get("positionAmt", 0))
side = p.get("positionSide") # LONG or SHORT
if abs(qty) < 1e-12:
continue
close_side = "SELL" if side == "LONG" else "BUY"
to_close.append((sym, side, abs(qty), close_side))


if not to_close:
print("No hedge legs to close.")
return


print("Will close:")
for sym, side, qty, close_side in to_close:
print(f" {sym} {side} qty={qty} via {close_side} MARKET")


if not confirm:
print("Dry run. Use --confirm to send orders.")
return


for sym, side, qty, close_side in to_close:
place_market_order(symbol=sym, side=close_side, quantity=qty, position_side=side)
print(f"Closed {sym} {side} qty={qty}")




if __name__ == "__main__":
ap = argparse.ArgumentParser()
ap.add_argument("symbols", nargs="*", help="optional symbols to flatten (default: all)")
ap.add_argument("--confirm", action="store_true", help="send orders")
args = ap.parse_args()
flatten(args.symbols, confirm=args.confirm)

## execution/hedge_sync.py
import os
import time

from execution.sync_state import sync_nav, sync_positions
from utils.firestore_client import get_db


def main():
    ENV = os.environ.get("ENV", "dev")
    db = get_db()
    while True:
        # TODO: replace with real readers for balances/positions from exchange or local state
        positions_payload = {"items": []}
        nav_payload = {
            "series": [],
            "total_equity": 0,
            "realized_pnl": 0,
            "unrealized_pnl": 0,
            "peak_equity": 0,
            "drawdown": 0,
        }
        try:
            sync_positions(db, positions_payload, ENV)
            sync_nav(db, nav_payload, ENV)
        except Exception as e:
            print(f"[hedge-sync] error: {e}")
        time.sleep(60)


if __name__ == "__main__":
    main()

## execution/leaderboard_sync.py
# execution/leaderboard_sync.py
import json
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, List

from firebase_admin import credentials, firestore, initialize_app

from execution.config_loader import get, load

STATE_FILE = "synced_state.json"
PEAK_FILE = "peak_state.json"


def _load_json(path: str) -> Any:
    if not Path(path).exists():
        return {} if path.endswith(".json") else []
    with open(path, "r") as f:
        return json.load(f)


def _pct(n, d):
    try:
        return float(n) / float(d) if float(d) != 0 else 0.0
    except:  # noqa: E722
        return 0.0


def compute_assets(state: dict) -> List[dict]:
    rows = []
    for sym, pos in state.items():
        qty = float(pos.get("qty") or 0.0)
        entry = float(pos.get("entry") or 0.0)
        last = float(pos.get("latest_price") or 0.0)
        pnl = (last - entry) * qty
        ret = _pct(last - entry, entry)
        rows.append(
            {
                "symbol": sym,
                "qty": qty,
                "entry": entry,
                "last": last,
                "unrealized": pnl,
                "return_pct": ret,
            }
        )
    rows.sort(key=lambda r: r["unrealized"], reverse=True)
    return rows


def compute_strategies(state: dict, peak: dict) -> List[dict]:
    # current value per strategy (sum MTM of owned symbols)
    values = {}
    for sym, pos in state.items():
        strat = pos.get("strategy") or "unknown"
        qty = float(pos.get("qty") or 0.0)
        last = float(pos.get("latest_price") or 0.0)
        values[strat] = values.get(strat, 0.0) + qty * last

    peaks = peak.get("strategies", {}) if isinstance(peak, dict) else {}
    rows = []
    for skey, val in values.items():
        pk = float(peaks.get(skey, 0.0))
        dd = 0.0 if pk == 0 else (val - pk) / pk
        rows.append(
            {
                "strategy": skey,
                "current_value": val,
                "peak_value": pk,
                "drawdown_pct": dd,
            }
        )
    rows.sort(key=lambda r: r["current_value"], reverse=True)
    return rows


def init_db():
    load()
    cred_path = get("runtime.FIREBASE_CREDS_PATH", "config/firebase_creds.json")
    if not Path(cred_path).exists():
        return None
    try:
        cred = credentials.Certificate(cred_path)
        initialize_app(cred)
        return firestore.client()
    except Exception:
        return None


def sync_once():
    if not get("execution.leaderboard_enabled", True):
        print("â„¹ï¸ Leaderboard disabled.")
        return False

    state = _load_json(STATE_FILE)
    peak = _load_json(PEAK_FILE)
    assets = compute_assets(state)
    strats = compute_strategies(state, peak)
    updated = datetime.now(timezone.utc).isoformat()

    db = init_db()
    if db is None:
        print("â„¹ï¸ No Firestore creds â€” skipping remote write (local only).")
        return False

    try:
        db.collection("hedge_leaderboard").document("assets").set(
            {"rows": assets, "updated_at": updated}
        )
        db.collection("hedge_leaderboard").document("strategies").set(
            {"rows": strats, "updated_at": updated}
        )
        print(f"âœ… Leaderboard synced at {updated}")
        return True
    except Exception as e:
        print(f"âŒ Leaderboard sync failed: {e}")
        return False


if __name__ == "__main__":
    while True:
        sync_once()
        time.sleep(300)

## execution/ml/__init__.py
__all__: list[str] = []

## execution/ml/data.py
from __future__ import annotations

import pandas as pd

import os
import numpy as np
from execution.exchange_utils import get_klines
# NOTE: Binance USD-M klines limit is 1500; paginate when requesting deeper history.


def load_candles(symbol: str, interval: str, limit: int) -> pd.DataFrame:
    if os.environ.get("ML_SIMULATOR", "0") == "1":
        freq = "H" if interval.endswith("h") or interval.endswith("H") else "H"
        idx = pd.date_range("2024-01-01", periods=limit, freq=freq, tz="UTC")
        drift = np.linspace(0, 0.003 * limit, limit)
        noise = np.random.normal(0, 0.2, size=limit).cumsum()
        base = 100 + drift + noise
        df = pd.DataFrame(
            {
                "open": base,
                "high": base * 1.001,
                "low": base * 0.999,
                "close": base,
                "volume": 1.0,
            },
            index=idx,
        )
        df.index.name = "open_time"
        return df

    rows = get_klines(symbol, interval, limit)
    if not rows:
        raise ValueError(f"no klines returned for {symbol} interval={interval}")
    cols = [
        "open_time",
        "open",
        "high",
        "low",
        "close",
        "volume",
    ]
    df = pd.DataFrame(rows)
    df = df.iloc[:, : len(cols)]
    df.columns = cols
    df["open_time"] = pd.to_datetime(df["open_time"], unit="ms", utc=True)
    for key in ["open", "high", "low", "close", "volume"]:
        df[key] = pd.to_numeric(df[key], errors="coerce")
    df = df.dropna().set_index("open_time").sort_index()
    return df

## execution/ml/features.py
import numpy as np
import pandas as pd

def talib_safe_rsi(close: pd.Series, n: int) -> pd.Series:
    delta = close.diff()
    gain = delta.clip(lower=0)
    loss = -delta.clip(upper=0)
    avg_gain = gain.ewm(alpha=1 / n, adjust=False).mean()
    avg_loss = loss.ewm(alpha=1 / n, adjust=False).mean()
    rs = avg_gain / (avg_loss.replace(0, np.nan))
    rsi = 100 - (100 / (1 + rs))
    return rsi.fillna(50)


def zscore(series: pd.Series, window: int) -> pd.Series:
    roll = series.rolling(window)
    return (series - roll.mean()) / (roll.std() + 1e-9)


def ema(series: pd.Series, span: int) -> pd.Series:
    return series.ewm(span=span, adjust=False).mean()


def build_feature_frame(df: pd.DataFrame, cfg: dict, ob_imbalance: float | None = None) -> pd.DataFrame:
    ml_cfg = cfg.get("ml", {})
    feat_cfg = ml_cfg.get("features", {})
    close = df["close"]
    feats = pd.DataFrame(index=df.index)
    feats["ret_1"] = close.pct_change().fillna(0.0)
    feats["ret_5"] = close.pct_change(5).fillna(0.0)
    feats["ema_fast"] = ema(close, feat_cfg.get("ema_fast", 20))
    feats["ema_slow"] = ema(close, feat_cfg.get("ema_slow", 50))
    feats["ema_diff"] = (feats["ema_fast"] - feats["ema_slow"]) / (close + 1e-9)
    feats["rsi"] = talib_safe_rsi(close, feat_cfg.get("rsi_len", 14)) / 100.0
    feats["zscore"] = (
        zscore(close.pct_change().fillna(0.0), feat_cfg.get("zscore_lookback", 48))
        .replace([np.inf, -np.inf], 0.0)
        .fillna(0.0)
    )
    if ob_imbalance is not None:
        feats["ob_imb_topn"] = float(ob_imbalance)
    return feats.replace([np.inf, -np.inf], 0.0).dropna().astype(float)


def make_labels(close: pd.Series, horizon: int, target_bps: float) -> pd.Series:
    forward_ret = close.shift(-horizon) / close - 1.0
    threshold = target_bps / 10000.0
    labels = (forward_ret >= threshold).astype(int)
    return labels

## execution/ml/predict.py
from __future__ import annotations

import json
import time
from pathlib import Path
from typing import Dict

import joblib

from execution.ml.data import load_candles
from execution.ml.features import build_feature_frame


def _load_registry(path: Path) -> Dict:
    try:
        return json.load(open(path, "r", encoding="utf-8"))
    except Exception:
        return {}


def score_symbol(cfg: Dict, symbol: str) -> Dict:
    ml_cfg = cfg.get("ml", {})
    if not ml_cfg.get("enabled", False):
        return {"enabled": False}

    registry_path = Path(ml_cfg.get("registry_path", "models/registry.json"))
    registry = _load_registry(registry_path)
    meta = registry.get(symbol)
    if not meta:
        return {"enabled": True, "error": "no_model"}

    model_blob = joblib.load(meta["model_path"])
    model = model_blob["model"]
    feature_names = model_blob["features"]

    candles = load_candles(symbol, ml_cfg.get("timeframe", "1h"), ml_cfg.get("lookback_bars", 2000))
    feats = build_feature_frame(candles, cfg).tail(1)
    if feats.empty:
        return {"enabled": True, "error": "no_features"}
    feats = feats[feature_names]
    prob = float(model.predict_proba(feats.values)[:, 1][0])

    return {
        "enabled": True,
        "symbol": symbol,
        "p": prob,
        "model": meta.get("model"),
        "auc": meta.get("auc"),
        "trained_at": meta.get("trained_at"),
        "ts": int(time.time()),
    }


def score_all(cfg: Dict) -> Dict[str, Dict]:
    results: Dict[str, Dict] = {}
    for sym in cfg.get("ml", {}).get("symbols", []):
        try:
            results[sym] = score_symbol(cfg, sym)
        except Exception as exc:
            results[sym] = {"error": str(exc)}
    return results


__all__ = ["score_symbol", "score_all"]

## execution/ml/train.py
from __future__ import annotations

import json
import time
from pathlib import Path
from typing import Dict, List

import joblib
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score

from execution.ml.data import load_candles
from execution.ml.features import build_feature_frame, make_labels


def _ts() -> int:
    return int(time.time())


def _ensure_dir(path: str) -> None:
    Path(path).parent.mkdir(parents=True, exist_ok=True)


def _train_logreg(X: np.ndarray, y: np.ndarray, cfg: Dict) -> LogisticRegression:
    params = {
        "max_iter": 200,
        "n_jobs": None,
        "class_weight": cfg.get("ml", {})
        .get("trainer", {})
        .get("class_weight", "balanced"),
    }
    model = LogisticRegression(**{k: v for k, v in params.items() if v is not None})
    model.fit(X, y)
    return model


def train_symbol(cfg: Dict, symbol: str) -> Dict:
    ml_cfg = cfg.get("ml", {})
    interval = ml_cfg.get("timeframe", "1h")
    lookback = int(ml_cfg.get("lookback_bars", 2000))
    horizon = int(ml_cfg.get("horizon_bars", 4))
    target_bps = float(ml_cfg.get("target_ret_bps", 20))

    candles = load_candles(symbol, interval, lookback + horizon + 20)
    feats = build_feature_frame(candles, cfg)
    labels = make_labels(candles["close"], horizon, target_bps).reindex(feats.index).dropna()
    feats = feats.loc[labels.index]
    if feats.empty:
        raise ValueError("insufficient features for training")

    X = feats.values
    y = labels.values.astype(int)
    test_size = float(ml_cfg.get("trainer", {}).get("test_size", 0.2))
    n_test = max(16, int(len(y) * test_size))
    if len(y) <= n_test:
        raise ValueError("not enough samples for holdout")
    X_train, X_test = X[:-n_test], X[-n_test:]
    y_train, y_test = y[:-n_test], y[-n_test:]

    model = _train_logreg(X_train, y_train, cfg)
    prob_test = model.predict_proba(X_test)[:, 1]
    auc = float(roc_auc_score(y_test, prob_test)) if len(set(y_test)) > 1 else float("nan")

    model_dir = Path("models")
    model_dir.mkdir(exist_ok=True)
    model_path = model_dir / f"{symbol}_logreg.pkl"
    joblib.dump({"model": model, "features": list(feats.columns)}, model_path)

    meta = {
        "symbol": symbol,
        "timeframe": interval,
        "lookback": lookback,
        "horizon": horizon,
        "target_bps": target_bps,
        "model": "logreg",
        "auc": auc,
        "trained_at": _ts(),
        "model_path": str(model_path),
    }

    registry_path = Path(ml_cfg.get("registry_path", "models/registry.json"))
    registry_path.parent.mkdir(parents=True, exist_ok=True)
    try:
        registry = json.load(open(registry_path, "r", encoding="utf-8"))
    except Exception:
        registry = {}
    registry[symbol] = meta
    with open(registry_path, "w", encoding="utf-8") as handle:
        json.dump(registry, handle, indent=2)
    return meta


def train_all(cfg: Dict) -> List[Dict]:
    ml_cfg = cfg.get("ml", {})
    registry_path = Path(ml_cfg.get("registry_path", "models/registry.json"))
    registry_path.parent.mkdir(parents=True, exist_ok=True)
    if not registry_path.exists():
        with open(registry_path, "w", encoding="utf-8") as handle:
            json.dump({}, handle)

    metas: List[Dict] = []
    for sym in ml_cfg.get("symbols", []):
        try:
            metas.append(train_symbol(cfg, sym))
        except Exception as exc:
            metas.append({"symbol": sym, "error": str(exc)})
    return metas


__all__ = ["train_symbol", "train_all"]

## execution/nav.py
from __future__ import annotations

import json
from typing import Any, Dict, Tuple

from execution.exchange_utils import get_balances, get_positions, get_price


def _load_json(path: str) -> Dict:
    try:
        with open(path, "r", encoding="utf-8") as handle:
            return json.load(handle)
    except Exception:
        return {}


def _futures_nav_usdt() -> Tuple[float, Dict]:
    """Compute USD-M futures wallet NAV and provide detail."""
    bal = get_balances() or {}
    # Support both dict-of-balances and list-of-dicts returns
    wallet = 0.0
    if isinstance(bal, dict):
        wallet = float(bal.get("USDT", bal.get("walletBalance", 0.0)) or 0.0)
    elif isinstance(bal, list):
        for entry in bal:
            try:
                if entry.get("asset") == "USDT":
                    wallet = float(entry.get("balance") or entry.get("walletBalance") or 0.0)
                    break
            except Exception:
                continue
    detail = {"futures_wallet_usdt": wallet}
    # Include unrealized PnL if present via positions
    try:
        positions = get_positions() or []
        unreal = 0.0
        for pos in positions:
            try:
                unreal += float(pos.get("unrealized", 0.0))
            except Exception:
                continue
        detail["unrealized_pnl"] = unreal
        wallet += unreal
    except Exception:
        pass
    return wallet, detail


_STABLES = {"USDT", "USDC", "BUSD"}


def _treasury_nav_usdt(treasury_path: str = "config/treasury.json") -> Tuple[float, Dict]:
    treasury = _load_json(treasury_path)
    if not treasury:
        return 0.0, {"treasury": {}}

    total = 0.0
    holdings: Dict[str, Dict[str, float]] = {}
    missing: Dict[str, str] = {}

    for asset, qty in treasury.items():
        if qty in (None, "", []):
            continue
        try:
            qty_float = float(qty)
        except Exception:
            continue
        if abs(qty_float) < 1e-12:
            continue

        asset_code = str(asset).upper()
        try:
            if asset_code in _STABLES:
                price = 1.0
            else:
                price = float(get_price(f"{asset_code}USDT"))
        except Exception as exc:
            missing[asset_code] = str(exc)
            continue

        value = qty_float * price
        total += value
        holdings[asset_code] = {
            "qty": qty_float,
            "px": price,
            "val_usdt": value,
        }

    breakdown: Dict[str, Any] = {"treasury": holdings, "total_treasury_usdt": total}
    if missing:
        breakdown["missing_prices"] = missing
    return total, breakdown


def _nav_sources(cfg: Dict) -> Tuple[str, str, bool, Any]:
    nav_cfg = cfg.get("nav") or {}
    trading_source = nav_cfg.get("trading_source") or nav_cfg.get("source") or "exchange"
    reporting_source = nav_cfg.get("reporting_source") or trading_source
    include_treasury = bool(nav_cfg.get("include_spot_treasury", False))
    manual = nav_cfg.get("manual_nav_usdt")
    return str(trading_source), str(reporting_source), include_treasury, manual


def _fallback_capital(cfg: Dict) -> Tuple[float, Dict]:
    fallback = float(cfg.get("capital_base_usdt", 0.0) or 0.0)
    return fallback, {"source": "capital_base"}


def compute_trading_nav(cfg: Dict) -> Tuple[float, Dict]:
    trading_source, _, _, manual = _nav_sources(cfg)
    if trading_source == "manual":
        if manual is not None:
            return float(manual), {"source": "manual"}
        return _fallback_capital(cfg)

    fut_nav, fut_detail = _futures_nav_usdt()
    if fut_nav > 0:
        return float(fut_nav), {"source": "exchange", **fut_detail}
    return _fallback_capital(cfg)


def compute_nav_summary(cfg: Dict | None = None) -> Dict[str, Any]:
    cfg = cfg or _load_json("config/strategy_config.json")

    futures_nav, futures_detail = _futures_nav_usdt()
    treasury_nav, treasury_detail = _treasury_nav_usdt()

    summary = {
        "futures_nav": float(futures_nav),
        "treasury_nav": float(treasury_nav),
        "total_nav": float(futures_nav + treasury_nav),
        "details": {
            "futures": futures_detail,
            "treasury": treasury_detail,
        },
    }
    return summary


def compute_reporting_nav(cfg: Dict) -> Tuple[float, Dict]:
    _, reporting_source, include_treasury, manual = _nav_sources(cfg)
    if reporting_source == "manual":
        if manual is not None:
            return float(manual), {"source": "manual"}
        return _fallback_capital(cfg)

    summary = compute_nav_summary(cfg)
    fut_detail = summary["details"].get("futures", {})
    detail: Dict[str, Any] = {"source": "exchange", **fut_detail}
    total_nav = float(summary["futures_nav"])

## execution/orderbook_features.py
from __future__ import annotations

import time
from typing import Any, Dict, Optional, Tuple

try:
    from .exchange_utils import _S, _BASE  # requests session/base
except Exception:  # pragma: no cover - offline
    _S = None
    _BASE = "https://fapi.binance.com"

_OB_CACHE: Dict[str, Tuple[float, Any]] = {}
_OB_TTL = 2.5  # seconds, rate-limit REST depth snapshots


def _fetch_depth(symbol: str, limit: int = 20) -> Optional[Dict[str, Any]]:
    if _S is None:  # offline/no-requests available
        return None
    url = f"{_BASE}/fapi/v1/depth?symbol={symbol}&limit={int(limit)}"
    r = _S.get(url, timeout=4)
    if not r.ok:
        return None
    return r.json()


def get_depth_snapshot(symbol: str, limit: int = 20) -> Optional[Dict[str, Any]]:
    now = time.time()
    key = f"{symbol}:{limit}"
    if key in _OB_CACHE:
        ts, data = _OB_CACHE[key]
        if (now - ts) <= _OB_TTL:
            return data
    data = _fetch_depth(symbol, limit=limit)
    _OB_CACHE[key] = (now, data)
    return data


def topn_imbalance(symbol: str, limit: int = 20) -> float:
    """Return simple buy-sell imbalance in top N levels: (bid_volume - ask_volume)/(sum).
    Positive => bid pressure; Negative => ask pressure. 0.0 on error.
    """
    try:
        d = get_depth_snapshot(symbol, limit=limit)
        if not d:
            return 0.0
        bids = sum(float(x[1]) for x in d.get("bids", [])[:limit])
        asks = sum(float(x[1]) for x in d.get("asks", [])[:limit])
        tot = max(1e-9, bids + asks)
        return (bids - asks) / tot
    except Exception:
        return 0.0


def evaluate_entry_gate(symbol: str, side: str, enabled: bool = True) -> Tuple[bool, Dict[str, Any]]:
    """Return (veto, info) based on simple imbalance gate.
    Veto on adverse pressure; allow slight boost when aligned.
    info = {"metric": float, "boost": float}
    """
    info: Dict[str, Any] = {"metric": 0.0, "boost": 0.0}
    if not enabled:
        return False, info
    m = topn_imbalance(symbol, limit=20)
    info["metric"] = m
    # thresholds tuned for micro-notional; keep conservative
    veto = False
    boost = 0.0
    side = str(side).upper()
    if side in ("BUY", "LONG") and m < -0.10:  # adverse ask pressure
        veto = True
    elif side in ("SELL", "SHORT") and m > 0.10:  # adverse bid pressure
        veto = True
    else:
        # aligned slight boost (<= +5%)
        if side in ("BUY", "LONG") and m > 0.20:
            boost = min(0.05, m)
        if side in ("SELL", "SHORT") and m < -0.20:
            boost = min(0.05, -m)
    info["boost"] = float(boost)
    return bool(veto), info

## execution/pipeline_probe.py
import hashlib
import hmac
import json
import math
import os
import time

import requests

from execution.signal_screener import generate_signals_from_config

BASE = "https://testnet.binancefuture.com"
AK = os.environ["BINANCE_API_KEY"]
SK = os.environ["BINANCE_API_SECRET"]


def sig(q):
    return hmac.new(SK.encode(), q.encode(), hashlib.sha256).hexdigest()


def get(path, **params):
    r = requests.get(
        BASE + path, params=params, headers={"X-MBX-APIKEY": AK}, timeout=10
    )
    r.raise_for_status()
    return r.json()


def post(path, params):
    q = "&".join(f"{k}={v}" for k, v in params.items())
    r = requests.post(
        BASE + path + "?" + q + "&signature=" + sig(q),
        headers={"X-MBX-APIKEY": AK},
        timeout=10,
    )
    print("POST", path, r.status_code, r.text)
    r.raise_for_status()
    return r.json()


def round_step(q, step):
    return math.floor(q / step) * step


def place_from_intent(intent):
    sym = intent["symbol"]
    px = float(get("/fapi/v1/ticker/price", symbol=sym)["price"])
    info = get("/fapi/v1/exchangeInfo")
    fx = {
        x["filterType"]: x
        for x in next(s for s in info["symbols"] if s["symbol"] == sym)["filters"]
    }
    step = float(fx["LOT_SIZE"]["stepSize"])
    min_q = float(fx["LOT_SIZE"]["minQty"])
    cap = float(intent.get("capital_per_trade", 0.0))
    lev = float(intent.get("leverage", 1.0))
    notional = max(1.0, cap * lev)
    qty = round_step(notional / px, step)
    if qty < min_q:
        qty = min_q

    ts = int(time.time() * 1000)
    side = "BUY" if intent["signal"] == "BUY" else "SELL"
    reduce = bool(intent.get("reduceOnly", False))

    # Hedge Mode: set positionSide based on intent + reduceOnly
    posSide = None
    if reduce:
        posSide = "LONG" if side == "SELL" else "SHORT"
    else:
        posSide = "LONG" if side == "BUY" else "SHORT"

    params = {
        "symbol": sym,
        "side": side,
        "type": "MARKET",
        "quantity": qty,
        "timestamp": ts,
        "positionSide": posSide,
    }
    print("INTENT:", json.dumps(intent, sort_keys=True))
    print("ORDER_PARAMS:", params, "px", px, "notional", notional)
    return post("/fapi/v1/order", params)


def main():
    placed = 0
    for intent in generate_signals_from_config() or []:
        # Only act on the first eligible intent to keep this probe safe/controlled.
        try:
            place_from_intent(intent)
            placed += 1
            break
        except Exception as e:
            print("ERR placing:", e)
    print("placed:", placed)


if __name__ == "__main__":
    main()

## execution/risk_limits.py
from dataclasses import dataclass
from decimal import ROUND_DOWN, Decimal, getcontext
from typing import Dict, List, Tuple, Optional, Any, Callable
import os
import time

from execution.nav import compute_trading_nav, compute_gross_exposure_usd
from execution.utils import load_json
from execution.exchange_utils import get_balances


_GLOBAL_KEYS = {
    "daily_loss_limit_pct",
    "cooldown_minutes_after_stop",
    "max_trades_per_symbol_per_hour",
    "drawdown_alert_pct",
    "max_gross_exposure_pct",
    "max_portfolio_gross_nav_pct",
    "max_symbol_exposure_pct",
    "min_notional_usdt",
    "max_trade_nav_pct",
    "max_concurrent_positions",
    "burst_limit",
    "error_circuit",
    "whitelist",
}


def _normalize_risk_cfg(cfg: Dict[str, Any] | None) -> Dict[str, Any]:
    """Ensure risk config exposes `global` and `per_symbol` sections."""
    if not isinstance(cfg, dict):
        return {"global": {}, "per_symbol": {}}

    out = dict(cfg)

    g = out.get("global")
    if not isinstance(g, dict):
        g = {}
    # hoist known globals if they were at top-level legacy locations
    for key in _GLOBAL_KEYS:
        if key in out and key not in g:
            g[key] = out[key]
    out["global"] = g

    per_symbol = out.get("per_symbol")
    if not isinstance(per_symbol, dict):
        per_symbol = {}
    out["per_symbol"] = per_symbol

    return out

getcontext().prec = 28


@dataclass(frozen=True)
class RiskConfig:
    max_notional_per_trade: float  # e.g., 200.0 USDT
    max_open_notional: float  # e.g., 1000.0 USDT
    max_positions: int  # e.g., 5
    max_leverage: float  # e.g., 5.0
    kill_switch_drawdown_pct: float  # e.g., -10.0 (portfolio)
    min_notional: float = 10.0  # exchange min


class RiskState:
    """Holds rolling state the executor can update each loop.

    Extended with lightweight fields/methods to support risk checks in `check_order`.
    """

    def __init__(self) -> None:
        self.open_notional: float = 0.0
        self.open_positions: int = 0
        self.portfolio_drawdown_pct: float = 0.0
        # New fields for cooldown/circuit breaker support
        self._last_fill_by_symbol: Dict[str, float] = {}
        self._error_timestamps: List[float] = []
        # Attempt timestamps for burst control
        self._order_attempt_ts: List[float] = []
        # Optional daily PnL percent (negative means loss)
        self.daily_pnl_pct: float = 0.0

    # --- Optional helpers used by check_order ---
    def note_fill(self, symbol: str, ts: float) -> None:
        self._last_fill_by_symbol[str(symbol)] = float(ts)

    def last_fill_ts(self, symbol: str) -> float:
        return float(self._last_fill_by_symbol.get(str(symbol), 0.0) or 0.0)

    def note_error(self, ts: float) -> None:
        self._error_timestamps.append(float(ts))

    def errors_in(self, window_sec: int, now: float) -> int:
        cutoff = float(now) - max(float(window_sec or 0), 0.0)
        kept = [t for t in self._error_timestamps if t >= cutoff]
        self._error_timestamps = kept
        return len(kept)

    def note_attempt(self, ts: float) -> None:
        self._order_attempt_ts.append(float(ts))

    def attempts_in(self, window_sec: int, now: float) -> int:
        cutoff = float(now) - max(float(window_sec or 0), 0.0)
        kept = [t for t in self._order_attempt_ts if t >= cutoff]
        self._order_attempt_ts = kept
        return len(kept)


def _can_open_position_legacy(
    symbol: str, notional: float, lev: float, cfg: RiskConfig, st: RiskState
) -> tuple[bool, str]:
    if st.portfolio_drawdown_pct <= cfg.kill_switch_drawdown_pct:
        return False, "kill_switch_triggered"
    if notional < cfg.min_notional:
        return False, "below_min_notional"
    if notional > cfg.max_notional_per_trade:
        return False, "exceeds_per_trade_cap"
    if lev > cfg.max_leverage:
        return False, "exceeds_leverage_cap"
    if (st.open_notional + notional) > cfg.max_open_notional:
        return False, "exceeds_open_notional_cap"
    if st.open_positions >= cfg.max_positions:
        return False, "too_many_positions"
    return True, "ok"


def can_open_position(*args, **kwargs):
    """
    Helper supporting two signatures for backward compatibility:
    - Legacy: can_open_position(symbol, notional, lev, cfg, st) -> (ok, reason)
    - New:    can_open_position(symbol, notional, lev, nav, open_qty, now, cfg, state, current_gross_notional=0.0)

    The new path delegates to check_order (if available) and returns (ok, first_reason_or_ok).
    """
    # Detect legacy 5-positional-args call used by current tests
    if len(args) == 5 and not kwargs:
        symbol, notional, lev, cfg, st = args
        return _can_open_position_legacy(symbol, notional, lev, cfg, st)

    # New signature path (allow both positional and keyword usage)
    symbol = kwargs.get("symbol", args[0] if len(args) > 0 else None)
    notional = kwargs.get("notional", args[1] if len(args) > 1 else None)
    lev = kwargs.get("lev", args[2] if len(args) > 2 else None)
    nav = kwargs.get("nav", args[3] if len(args) > 3 else None)
    open_qty = kwargs.get("open_qty", args[4] if len(args) > 4 else None)
    now = kwargs.get("now", args[5] if len(args) > 5 else None)
    cfg = kwargs.get("cfg", args[6] if len(args) > 6 else None)
    state = kwargs.get("state", args[7] if len(args) > 7 else None)
    current_gross_notional = kwargs.get(
        "current_gross_notional", args[8] if len(args) > 8 else 0.0

## execution/rules_sl_tp.py
#!/usr/bin/env python3
"""
Pure SL/TP rules for the hedge bot.

Interfaces:
- compute_sl_tp(entry_px, side, atr, atr_mult, fixed_sl_pct, fixed_tp_pct, trail=None)
- should_exit(prices, entry_px, side, sl_px, tp_px, max_bars, trail=None)

Conventions:
- `side`: "LONG" or "SHORT".
- `atr`: fraction of price (e.g., 0.0025 == 0.25%).
- `fixed_sl_pct` / `fixed_tp_pct` may be given either as percent (e.g., 0.6 => 0.6%)
  **or** as a fraction (e.g., 0.006). We auto-normalize:
    * <= 0.05  -> treated as fraction (<= 5%)
    *  > 0.05  -> treated as percent and divided by 100
"""

from typing import Dict, Iterable, Optional, Tuple


def _normalize_pct(x: float) -> float:
    """
    Interprets values <= 0.05 as already-fractions (<= 5%),
    and larger values as percents that should be divided by 100.
    """
    x = float(x or 0.0)
    if x <= 0.0:
        return 0.0
    if x <= 0.05:
        return x  # already a fraction (e.g., 0.006 => 0.6%)
    return x / 100.0  # percent (e.g., 0.6 => 0.006)


def compute_sl_tp(
    entry_px: float,
    side: str,
    atr: float = 0.0,
    atr_mult: float = 0.0,
    fixed_sl_pct: float = 0.0,
    fixed_tp_pct: float = 0.0,
    trail: Optional[Dict] = None,
) -> Tuple[float, float]:
    """
    Returns (sl_px, tp_px). No tick-size rounding here.
    """
    side = side.upper()

    sl_pct = _normalize_pct(fixed_sl_pct)
    tp_pct = _normalize_pct(fixed_tp_pct)

    atr = float(atr or 0.0)  # ATR already a fraction
    atr_mult = float(atr_mult or 0.0)
    if atr > 0.0 and atr_mult > 0.0:
        atr_component = atr * atr_mult
        sl_pct = max(sl_pct, atr_component)
        tp_pct = max(tp_pct, atr_component)

    if side == "LONG":
        sl_px = entry_px * (1.0 - sl_pct) if sl_pct > 0 else entry_px
        tp_px = entry_px * (1.0 + tp_pct) if tp_pct > 0 else entry_px
    elif side == "SHORT":
        sl_px = entry_px * (1.0 + sl_pct) if sl_pct > 0 else entry_px
        tp_px = entry_px * (1.0 - tp_pct) if tp_pct > 0 else entry_px
    else:
        raise ValueError("side must be 'LONG' or 'SHORT'")
    return float(sl_px), float(tp_px)


def should_exit(
    prices: Iterable[float],
    entry_px: float,
    side: str,
    sl_px: float,
    tp_px: float,
    max_bars: int,
    trail: Optional[Dict] = None,
) -> bool:
    """
    Evaluate exit conditions on a chronological sequence of prices.
    True if TP/SL/TRAIL/TIME condition is met.
    """
    side = side.upper()
    it = [float(x) for x in prices]
    if not it:
        return False
    last = it[-1]

    # Hard TP/SL
    if side == "LONG":
        if last >= tp_px:
            return True  # take profit
        if last <= sl_px:
            return True  # stop loss
    elif side == "SHORT":
        if last <= tp_px:
            return True
        if last >= sl_px:
            return True
    else:
        raise ValueError("side must be 'LONG' or 'SHORT'")

    # Trailing stop (optional)
    if trail and float(trail.get("width_pct", 0)) > 0:
        w = float(trail["width_pct"])
        if side == "LONG":
            peak = max(it)
            trail_px = peak * (1.0 - w)
            if last <= trail_px:
                return True
        else:  # SHORT
            trough = min(it)
            trail_px = trough * (1.0 + w)
            if last >= trail_px:
                return True

    # Time stop
    if max_bars and len(it) >= int(max_bars):
        return True

    return False

## execution/signal_doctor.py
#!/usr/bin/env python3
from __future__ import annotations

import argparse
import json
import os
import sys
import time
from typing import Any, Dict, List

from .risk_limits import RiskState, check_order
from .universe_resolver import symbol_tier, is_listed_on_futures

try:
    from .exchange_utils import get_price, get_symbol_filters
except Exception:
    # Allow running without network: stub minimal behavior
    def get_price(symbol: str) -> float:
        return 0.0

    def get_symbol_filters(symbol: str) -> Dict[str, Any]:
        raise RuntimeError("exchange access unavailable")


WL_DEFAULT = [
    "BTCUSDT",
    "ETHUSDT",
    "SOLUSDT",
    "LINKUSDT",
    "BNBUSDT",
    "SUIUSDT",
    "LTCUSDT",
    "WIFUSDT",
    "DOGEUSDT",
    "ARBUSDT",
    "OPUSDT",
    "AVAXUSDT",
    "APTUSDT",
    "INJUSDT",
    "RNDRUSDT",
    "SEIUSDT",
    "TONUSDT",
    "XRPUSDT",
    "ADAUSDT",
]


def _load_cfg() -> Dict[str, Any]:
    try:
        return json.load(open("config/risk_limits.json"))
    except Exception:
        return {"global": {}, "per_symbol": {}}


def _nav_snapshot_fallback() -> float:
    try:
        # Best effort from executor's account snapshot if available
        from .executor_live import _compute_nav

        return float(_compute_nav())
    except Exception:
        return 1000.0  # safe placeholder


def diagnose_symbols(env: str, testnet: bool, symbols: List[str]) -> int:
    cfg = _load_cfg()
    g = cfg.get("global") or {}
    wl = g.get("whitelist") or WL_DEFAULT
    wl_set = {str(x).upper() for x in wl}
    now = time.time()
    nav = _nav_snapshot_fallback()
    st = RiskState()

    print(f"[doctor] ENV={env} testnet={int(testnet)} nav={nav:.2f}")
    exit_code = 0
    for sym in symbols:
        s = str(sym).upper()
        if s not in wl_set:
            print(
                json.dumps(
                    {
                        "symbol": s,
                        "would_emit": False,
                        "would_block": True,
                        "reasons": ["not_whitelisted"],
                    }
                )
            )
            continue
        # Defaults
        exch_min_notional = 0.0
        price = 0.0
        listed = is_listed_on_futures(s)
        if listed:
            try:
                price = float(get_price(s))
                f = get_symbol_filters(s)
                exch_min_notional = float(
                    (f.get("MIN_NOTIONAL", {}) or {}).get("notional", 0) or 0.0
                )
            except Exception:
                listed = False

        # Strategy leverage and capital from strategy_config.json (fallbacks)
        cap = 10.0
        lev = float(g.get("max_leverage", 20) or 20)
        try:
            scfg = json.load(open("config/strategy_config.json"))
            arr = scfg.get("strategies", []) if isinstance(scfg, dict) else []
            for row in arr:
                if isinstance(row, dict) and str(row.get("symbol", "")).upper() == s:
                    cap = float(row.get("capital_per_trade", cap) or cap)
                    lev = float(row.get("leverage", lev) or lev)
                    break
        except Exception:
            pass

        vetoes: List[str] = []
        if not listed:
            vetoes.append("not_listed")

        # Risk check (gross notional = cap)
        # compute current gross for portfolio/tier (doctor is stateless; pass 0)
        tier = symbol_tier(s) or "UNKNOWN"
        ok, details = check_order(
            symbol=s,
            side="BUY",
            requested_notional=cap,
            price=price,
            nav=nav,
            open_qty=0.0,
            now=now,
            cfg=cfg,
            state=st,
            current_gross_notional=0.0,
            lev=lev,
            open_positions_count=0,
            tier_name=tier,
            current_tier_gross_notional=0.0,
        )
        reasons = list(details.get("reasons", [])) if isinstance(details, dict) else []
        vetoes.extend([r for r in reasons if r not in vetoes])
        would_emit = listed and ok and (
            cap >= max(exch_min_notional, float(g.get("min_notional_usdt", 0) or 0))
        )
        # Include budget info for readout
        tcfg = (g.get("tiers") or {}).get(tier, {}) if isinstance(g, dict) else {}
        out = {
            "symbol": s,
            "price": price,

## execution/signal_screener.py
#!/usr/bin/env python3
from __future__ import annotations

import json
import time
from typing import Any, Dict, Iterable, List, Tuple

import os
from .exchange_utils import get_klines, get_price, get_symbol_filters
from .orderbook_features import evaluate_entry_gate
from .universe_resolver import resolve_allowed_symbols, symbol_tier, is_listed_on_futures
from .risk_limits import check_order, RiskState, RiskGate
from .nav import PortfolioSnapshot

try:
    from .ml.predict import score_symbol as _score_symbol
except Exception:  # pragma: no cover - optional dependency
    _score_symbol = None

LOG_TAG = "[screener]"


def _positions_by_side(
    positions: Iterable[Dict[str, Any]]
) -> Dict[str, Dict[str, Dict[str, float]]]:
    out: Dict[str, Dict[str, Dict[str, float]]] = {}
    for raw in positions or []:
        try:
            sym = str(raw.get("symbol", "")).upper()
            if not sym:
                continue
            side = str(raw.get("positionSide", "BOTH")).upper()
            if side not in ("LONG", "SHORT"):
                continue
            qty = float(raw.get("qty", raw.get("positionAmt", 0.0)) or 0.0)
            if qty == 0.0:
                continue
            abs_qty = abs(qty)
            mark = float(raw.get("markPrice") or raw.get("entryPrice") or 0.0)
            notional = abs_qty * abs(mark)
            out.setdefault(sym, {})[side] = {
                "qty": abs_qty,
                "mark": abs(mark),
                "notional": notional,
            }
        except Exception:
            continue
    return out


def _reduce_plan(
    symbol: str,
    signal: str,
    timeframe: str,
    positions: Dict[str, Dict[str, float]],
    fallback_price: float,
) -> Tuple[List[Dict[str, Any]], float]:
    """Return (reduce_intents, notional_delta)."""
    desired_side = "LONG" if signal == "BUY" else "SHORT"
    opposite_side = "SHORT" if desired_side == "LONG" else "LONG"
    opp = positions.get(opposite_side, {}) if positions else {}
    qty = float(opp.get("qty", 0.0) or 0.0)
    if qty <= 0.0:
        return [], 0.0
    mark = float(opp.get("mark", 0.0) or 0.0)
    mark = mark if mark > 0 else float(fallback_price)
    if mark <= 0:
        return [], 0.0
    notional = float(opp.get("notional", qty * mark) or (qty * mark))
    reduce_signal = "BUY" if opposite_side == "SHORT" else "SELL"
    intent = {
        "timestamp": time.strftime("%Y-%m-%dT%H:%M:%S+00:00", time.gmtime()),
        "symbol": symbol,
        "timeframe": timeframe,
        "signal": reduce_signal,
        "reduceOnly": True,
        "positionSide": opposite_side,
        "price": mark,
        "capital_per_trade": notional,
        "leverage": 1.0,
        "gross_usd": notional,
        "source": "auto_reduce",
    }
    return [intent], notional


_SCREENER_RISK_STATE = RiskState()
_SCREENER_GATE = RiskGate({"sizing": {}, "risk": {}})


def _update_screener_risk_state(
    snapshot: PortfolioSnapshot,
    open_notional: float,
    open_positions: int,
) -> None:
    _SCREENER_GATE.nav_provider = snapshot
    try:
        loss_pct = float(_SCREENER_GATE._daily_loss_pct())
    except Exception:
        loss_pct = 0.0
    _SCREENER_RISK_STATE.daily_pnl_pct = -loss_pct
    _SCREENER_RISK_STATE.open_notional = float(open_notional)
    _SCREENER_RISK_STATE.open_positions = int(open_positions)


def _risk_cfg_path() -> str:
    return os.getenv("RISK_LIMITS_CONFIG", "config/risk_limits.json")


def _load_strategy_list() -> List[Dict[str, Any]]:
    scfg = json.load(open("config/strategy_config.json"))
    raw = scfg.get("strategies", scfg)
    lst = (
        raw
        if isinstance(raw, list)
        else (list(raw.values()) if isinstance(raw, dict) else [])
    )
    # Prefer universe_resolver for allowed symbols (handles listing + throttles)
    try:
        allowed, _ = resolve_allowed_symbols()
        allow_set = {s.upper() for s in (allowed or [])}
        if allow_set:
            lst = [
                s
                for s in lst
                if isinstance(s, dict)
                and str(s.get("symbol", "")).upper() in allow_set
            ]
    except Exception:
        # If resolver fails (offline), fall through; we'll veto not_listed later
        pass
    return [s for s in lst if isinstance(s, dict) and s.get("enabled")]


def _load_risk_cfg() -> Dict[str, Any]:
    try:
        return json.load(open(_risk_cfg_path()))
    except Exception:
        return {}


def would_emit(
    symbol: str,
    side: str,
    *,
    notional: float = 10.0,
    lev: float = 20.0,
    nav: float = 1000.0,
    open_positions_count: int = 0,
    current_gross_notional: float = 0.0,

## execution/state_publish.py
#!/usr/bin/env python3
from __future__ import annotations

import hashlib
import json

# Publishes read-only state to Firestore (positions + NAV).
# - Loads .env from repo root (override=True) so ad-hoc runs see keys.
# - Filters positions to non-zero qty and to symbols in pairs_universe.json (if present).
# - Debounces writes (executor can call this every loop safely).
import os
import pathlib
import time
from typing import Any, Dict, List, Optional

# ----- robust .env load -----
ROOT_DIR = pathlib.Path(__file__).resolve().parents[1]
try:
    from dotenv import load_dotenv

    load_dotenv(ROOT_DIR / ".env", override=True)
except Exception:
    pass

ENV = os.getenv("ENV", "prod")
FS_ROOT = f"hedge/{ENV}/state"
CREDS_PATH = os.getenv("GOOGLE_APPLICATION_CREDENTIALS") or str(
    ROOT_DIR / "config/firebase_creds.json"
)
LOG_DIR = ROOT_DIR / "logs"


def _ensure_keys() -> None:
    """Backstop parse of .env in case python-dotenv wasn't available."""
    if os.getenv("BINANCE_API_KEY") and os.getenv("BINANCE_API_SECRET"):
        return
    env_path = ROOT_DIR / ".env"
    if not env_path.exists():
        return
    for ln in env_path.read_text().splitlines():
        ln = ln.strip()
        if not ln or ln.startswith("#") or "=" not in ln:
            continue
        k, v = ln.split("=", 1)
        os.environ.setdefault(k.strip(), v.strip())


_ensure_keys()


def _firestore_enabled() -> bool:
    return os.environ.get("FIRESTORE_ENABLED", "1") != "0"


# ----- Firestore client -----
def _fs_client():
    if not _firestore_enabled():
        raise RuntimeError("firestore_disabled")
    from google.cloud import firestore

    if os.path.exists(CREDS_PATH):
        from google.oauth2 import service_account

        info = json.load(open(CREDS_PATH))
        creds = service_account.Credentials.from_service_account_file(CREDS_PATH)
        return firestore.Client(project=info.get("project_id"), credentials=creds)
    return firestore.Client()


# ----- Exchange helpers (import late so .env is loaded) -----
def get_positions(symbol: Optional[str] = None) -> List[Dict[str, Any]]:
    from execution.exchange_utils import _req

    params = {}
    if symbol:
        params["symbol"] = symbol
    return _req("GET", "/fapi/v2/positionRisk", signed=True, params=params).json()


def get_account() -> Dict[str, Any]:
    from execution.exchange_utils import _req

    return _req("GET", "/fapi/v2/account", signed=True).json()


# ----- Normalizers / publishers -----
def normalize_positions(raw: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    # Optional universe whitelist
    universe = None
    try:
        u = json.load(open(str(ROOT_DIR / "config/pairs_universe.json")))
        universe = set(u.get("symbols", []))
    except Exception:
        pass

    rows: List[Dict[str, Any]] = []
    for p in raw or []:
        try:
            sym = p.get("symbol")
            if universe and sym not in universe:
                continue
            qty = float(p.get("qty", p.get("positionAmt", 0)) or 0.0)
            if qty == 0.0:
                continue
            rows.append(
                {
                    "symbol": sym,
                    "positionSide": p.get("positionSide") or "BOTH",
                    "qty": qty,
                    "entryPrice": float(p.get("entryPrice") or 0),
                    "leverage": float(p.get("leverage") or 0),
                    "uPnl": float(
                        p.get("unRealizedProfit", p.get("unrealized", 0)) or 0
                    ),
                }
            )
        except Exception:
            pass
    return rows


def publish_positions(rows: List[Dict[str, Any]]) -> None:
    payload = {"rows": rows, "updated": time.time()}
    if not _firestore_enabled():
        _append_local_jsonl("positions", payload)
        return
    cli = _fs_client()
    cli.document(f"{FS_ROOT}/positions").set(payload, merge=True)


def compute_nav() -> float:
    acc = get_account()
    # Prefer totalMarginBalance (wallet + uPnL)
    tmb = acc.get("totalMarginBalance")
    if tmb is not None:
        return float(tmb)
    twb = float(acc.get("totalWalletBalance", 0) or 0)
    tup = float(acc.get("totalUnrealizedProfit", 0) or 0)
    return twb + tup


def publish_nav_value(
    nav: float, min_interval_s: int = 60, max_points: int = 20000
) -> None:
    now = time.time()
    if not _firestore_enabled():
        _append_local_jsonl("nav", {"t": now, "nav": float(nav)})
        return
    cli = _fs_client()
    doc = cli.document(f"{FS_ROOT}/nav")

## execution/sync_daemon.py
import json
import os
import sys
import time
import traceback
from datetime import datetime, timezone

from execution.exchange_utils import get_positions  # fallback if no local positions
from execution.sync_state import sync_leaderboard, sync_nav, sync_positions
from utils.firestore_client import get_db

LEADERBOARD_FILE = "leaderboard.json"
NAV_FILE = "nav_log.json"
PEAK_FILE = "peak_state.json"
STATE_FILE = "synced_state.json"  # adjust if your project uses a different name

INTERVAL = int(os.getenv("SYNC_INTERVAL_SEC", "15"))


def load_json_safe(path, default):
    try:
        with open(path, "r") as f:
            return json.load(f)
    except Exception:
        return default


def now_iso():
    return datetime.now(timezone.utc).isoformat()


def build_data_payload():
    # Leaderboard: list of rows or empty list
    leaderboard = load_json_safe(LEADERBOARD_FILE, [])

    # NAV: series (list of {t, equity}) and peak (float)
    series = load_json_safe(NAV_FILE, [])
    peak_state = load_json_safe(PEAK_FILE, {})
    peak = peak_state.get("peak")
    if peak is None:
        try:
            peak = max((pt.get("equity", 0.0) for pt in series), default=0.0)
        except Exception:
            peak = 0.0

    nav = {
        "series": series,
        "peak": peak,
        "updated_at": now_iso(),
    }

    # Positions: prefer local state file; fallback to live exchange query
    state = load_json_safe(STATE_FILE, {})
    positions = state.get("positions")
    if not isinstance(positions, list):
        try:
            positions = get_positions()
        except Exception:
            positions = []

    positions_payload = {
        "rows": positions,
        "updated_at": now_iso(),
    }

    return {
        "leaderboard": leaderboard,
        "nav": nav,
        "positions": positions_payload,
    }


def run_once(db, env):
    data = build_data_payload()
    # sync_* expect (db, data, env)
    sync_leaderboard(db, data, env)
    sync_nav(db, data, env)
    sync_positions(db, data, env)


if __name__ == "__main__":
    db = get_db()
    env = os.getenv("ENV", "prod")
    while True:
        try:
            run_once(db, env)
            sys.stdout.write("âœ” sync ok\n")
            sys.stdout.flush()
        except Exception:
            traceback.print_exc()
            time.sleep(5)
        time.sleep(INTERVAL)

## execution/sync_state.py
from __future__ import annotations

import json

# execution/sync_state.py â€” Phaseâ€‘4.1 â€œStability & Signalsâ€ (hardened sync)
#
# What this does
#  - Reads local files (nav_log.json, peak_state.json, synced_state.json)
#  - Applies cutoff filtering to NAV history if configured
#  - Guards against zero-equity rows and empty tails
#  - Computes exposure KPIs from positions
#  - Derives peak from best available source (file, rows, existing Firestore doc)
#  - Upserts compact docs to Firestore:
#       hedge/{ENV}/state/nav
#       hedge/{ENV}/state/positions
#       hedge/{ENV}/state/leaderboard
#
# Env knobs
#  ENV=prod|dev
#  NAV_CUTOFF_ISO="2025-08-01T00:00:00+00:00"   # preferred explicit cutoff
#  NAV_CUTOFF_SECAGO=86400                       # or relative cutoff in seconds
#  SYNC_INTERVAL_SEC=20
#
import os
import time
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, List, Optional, Tuple

# ---------------- Firestore import (supports multiple layouts) ---------------
try:
    # legacy: project root
    from firestore_client import get_db  # type: ignore
except ModuleNotFoundError:  # pragma: no cover
    try:
        # sibling package layout
        from utils.firestore_client import get_db  # type: ignore
    except ModuleNotFoundError:
        # monorepo package layout
        from hedge_fund.utils.firestore_client import get_db  # type: ignore

# ------------------------------- Files ---------------------------------------
NAV_LOG: str = "nav_log.json"
PEAK_STATE: str = "peak_state.json"
SYNCED_STATE: str = "synced_state.json"

# ------------------------------ Settings ------------------------------------
MAX_POINTS: int = 500  # dashboard series cap


# ------------------------------- Utilities ----------------------------------


def _now_iso() -> str:
    return datetime.now(timezone.utc).isoformat()


def _parse_iso(ts: str) -> Optional[datetime]:
    if not ts:
        return None
    try:
        if ts.endswith("Z"):
            ts = ts.replace("Z", "+00:00")
        return datetime.fromisoformat(ts)
    except Exception:
        return None


def _cutoff_dt() -> Optional[datetime]:
    """Cutoff datetime from env: NAV_CUTOFF_ISO (preferred) or NAV_CUTOFF_SECAGO."""
    iso = os.getenv("NAV_CUTOFF_ISO")
    if iso:
        dt = _parse_iso(iso)
        if dt:
            return dt
    sec = os.getenv("NAV_CUTOFF_SECAGO")
    if sec:
        try:
            s = int(sec)
            return datetime.now(timezone.utc) - timedelta(seconds=s)
        except Exception:
            pass
    return None


# ----------------------------- File readers ---------------------------------


def _read_nav_rows(path: str) -> List[Dict[str, Any]]:
    """Read nav_log.json list and normalize timestamps to key 't'."""
    rows: List[Dict[str, Any]] = []
    if not os.path.exists(path):
        return rows
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        if isinstance(data, list):
            cut = _cutoff_dt()
            for p in data:
                # normalize timestamp key for series
                ts = p.get("timestamp") or p.get("t")
                if ts and "t" not in p:
                    p = {**p, "t": ts}
                # enforce cutoff if configured
                if cut:
                    dt = _parse_iso(p.get("t") or "")
                    if dt and dt < cut:
                        continue
                rows.append(p)
    except Exception:
        pass
    return rows


def _read_peak_file(path: str) -> float:
    if not os.path.exists(path):
        return 0.0
    try:
        with open(path, "r", encoding="utf-8") as f:
            j = json.load(f) or {}
        return float(j.get("peak_equity") or 0.0)
    except Exception:
        return 0.0


def _read_positions_snapshot(path: str) -> Dict[str, Any]:
    if not os.path.exists(path):
        return {"items": [], "updated_at": _now_iso()}
    try:
        with open(path, "r", encoding="utf-8") as f:
            j = json.load(f) or {}
        items = j.get("items") or []
        return {"items": items, "updated_at": j.get("updated_at") or _now_iso()}
    except Exception:
        return {"items": [], "updated_at": _now_iso()}


# --- KPI tail reader for nav card metrics
def _read_nav_tail_metrics(path: str) -> Dict[str, float]:
    """Return last point's metrics for nav KPIs (safe defaults)."""
    out = {
        "total_equity": 0.0,
        "realized_pnl": 0.0,
        "unrealized_pnl": 0.0,
        "drawdown": 0.0,
    }
    if not os.path.exists(path):
        return out
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)

## execution/telegram_report.py
#!/usr/bin/env python3
from __future__ import annotations

import argparse
import os
import sys
import time
from typing import Any, Dict, List, Tuple


def _to_epoch(t: Any) -> float:
    if isinstance(t, (int, float)):
        x = float(t)
        return x / 1000.0 if x > 1e12 else x
    if hasattr(t, "timestamp"):
        try:
            return float(t.timestamp())
        except Exception:
            return 0.0
    return 0.0


def _load_nav_series(env: str) -> List[Dict[str, Any]]:
    try:
        from utils.firestore_client import get_db

        db = get_db()
        doc = db.collection("hedge").document(env).collection("state").document("nav").get()
        data = doc.to_dict() if getattr(doc, "exists", False) else {}
        out: List[Dict[str, Any]] = []
        for _, v in (data.items() if isinstance(data, dict) else []):
            if not isinstance(v, dict):
                continue
            ts = _to_epoch(v.get("t") or v.get("time") or v.get("ts"))
            if ts <= 0:
                continue
            val = v.get("nav") or v.get("value") or v.get("equity") or v.get("v")
            if not isinstance(val, (int, float, str)):
                continue
            nav = float(val)
            out.append({"ts": ts, "nav": nav})
        out.sort(key=lambda r: r["ts"])
        return out
    except Exception:
        return []


def _compose_ai_note() -> str:
    key = os.getenv("OPENAI_API_KEY", "")
    if not key:
        # Safe fallback note
        return (
            "Market note: BTC, ETH, SOL saw typical twoâ€‘sided flows over the last 24h. "
            "Liquidity conditions appear normal; no material dislocations observed. This is not advice."
        )
    try:
        import openai

        client = openai.OpenAI(api_key=key)
        prompt = (
            "Write an 80â€“120 word neutral market note focused on BTC/ETH/SOL drivers in the last 24h. "
            "Avoid hype or recommendations. Close with: 'This is not investment advice.'"
        )
        resp = client.chat.completions.create(
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=160,
        )
        text = resp.choices[0].message.content.strip()
        return text
    except Exception:
        return (
            "Market note: BTC, ETH, SOL traded mixed with modest rotation; "
            "derivatives premiums and funding were stable. This is not investment advice."
        )


def _save_nav_png(series: List[Dict[str, Any]], path: str = "/tmp/nav.png") -> str:
    try:
        import matplotlib
        matplotlib.use("Agg")
        import matplotlib.pyplot as plt

        xs = [r["ts"] for r in series]
        ys = [r["nav"] for r in series]
        fig, ax = plt.subplots(figsize=(3.2, 2.0), dpi=160)
        ax.plot(xs, ys, color="#1f77b4", linewidth=1.2)
        ax.set_title("NAV (7d)", fontsize=8)
        ax.grid(True, alpha=0.2)
        for spine in ("top", "right"):
            ax.spines[spine].set_visible(False)
        plt.tight_layout()
        fig.savefig(path)
        plt.close(fig)
        return path
    except Exception:
        # Fallback: write a minimal 1x1 PNG placeholder
        png_minimal = (
            b"\x89PNG\r\n\x1a\n\x00\x00\x00\rIHDR\x00\x00\x00\x01\x00\x00\x00\x01\x08\x06\x00\x00\x00\x1f\x15\xc4\x89"
            b"\x00\x00\x00\x0cIDAT\x08\x99c\xf8\xff\xff?\x00\x05\xfe\x02\xfeA\x0f\xeb\xb1\x00\x00\x00\x00IEND\xaeB`\x82"
        )
        try:
            with open(path, "wb") as f:
                f.write(png_minimal)
            return path
        except Exception as e:
            raise RuntimeError(f"png write error: {e}")


def _counts_24h(env: str) -> Tuple[int, str]:
    """Return (trades_24h, top_veto_reason) from Firestore, best-effort."""
    trades_24h = 0
    top_reason = ""
    try:
        from utils.firestore_client import get_db

        db = get_db()
        root = db.collection("hedge").document(env)
        now = time.time()

        # Trades
        docs_t = list(
            root.collection("trades").order_by("ts", direction="DESCENDING").limit(1000).stream()
        )
        ts_ok = 0
        for d in docs_t:
            x = d.to_dict() or {}
            t = _to_epoch(x.get("ts") or x.get("time"))
            if (now - t) <= 24 * 3600:
                ts_ok += 1
        trades_24h = ts_ok

        # Risk veto top
        docs_r = list(
            root.collection("risk").order_by("ts", direction="DESCENDING").limit(1000).stream()
        )
        counts: Dict[str, int] = {}
        for d in docs_r:
            x = d.to_dict() or {}
            t = _to_epoch(x.get("ts") or x.get("time"))
            if (now - t) > 24 * 3600:
                continue
            reasons_val = x.get("reasons")
            if isinstance(reasons_val, list):
                for r in reasons_val:
                    key = str(r)
                    counts[key] = counts.get(key, 0) + 1
            elif x.get("reason") is not None:
                key = str(x.get("reason"))

## execution/telegram_utils.py
from __future__ import annotations

# execution/telegram_utils.py â€” Phase 4.1
import os
import time
from datetime import datetime
from typing import List

try:
    import requests
except Exception:
    requests = None  # handled below


# --- Env helpers ---
def _b(x: str) -> bool:
    return str(x).strip().lower() in ("1", "true", "yes", "on")


def _env():
    return {
        "enabled": _b(os.getenv("TELEGRAM_ENABLED", "0")),
        "token": os.getenv("BOT_TOKEN", "").strip(),
        "chat": os.getenv("CHAT_ID", "").strip(),
    }


def _utc() -> str:
    return datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S UTC")


# --- Core send ---
def send_telegram(message: str, silent: bool = False) -> bool:
    env = _env()
    if not env["enabled"]:
        print("âŒ Telegram disabled (TELEGRAM_ENABLED!=1).", flush=True)
        return False
    if not env["token"] or not env["chat"]:
        print(
            f"âŒ Telegram missing creds (BOT_TOKEN len={len(env['token'])}, CHAT_ID set={bool(env['chat'])}).",
            flush=True,
        )
        return False
    if requests is None:
        print("âŒ Telegram cannot import requests.", flush=True)
        return False
    try:
        url = f"https://api.telegram.org/bot{env['token']}/sendMessage"
        payload = {
            "chat_id": env["chat"],
            "text": f"{_utc()}\n{message}",
            "disable_notification": bool(silent),
        }
        r = requests.post(url, json=payload, timeout=15)
        if r.ok:
            print("âœ… Telegram message sent.", flush=True)
            return True
        print(f"âŒ Telegram send failed [{r.status_code}]: {r.text}", flush=True)
        return False
    except Exception as e:
        print(f"âŒ Telegram send error: {e}", flush=True)
        return False


# --- Cadence / rate limiting ---
_last_summary_ts: float | None = None
_last_dd_ts: float | None = None


def should_send_summary(last_sent_ts: float | None, minutes: int) -> bool:
    now = time.time()
    if not last_sent_ts:
        return True
    return (now - float(last_sent_ts)) >= max(60, minutes * 60)


# --- Message helpers used by executor_live.py ---
def send_heartbeat(
    equity: float,
    peak: float,
    dd_pct: float,
    realized: float,
    unrealized: float,
    positions_top: List[str],
):
    msg = (
        f"Heartbeat\n"
        f"Equity: {equity:,.2f} | Peak: {peak:,.2f} | DD: {dd_pct * 100:+.2f}%\n"
        f"PnL â€” R: {realized:,.2f} | U: {unrealized:,.2f}\n"
        f"Top: {', '.join(positions_top) if positions_top else 'â€”'}"
    )
    send_telegram(msg, silent=True)


def send_trade_alert(
    symbol: str,
    side: str,
    qty: float,
    fill_price: float,
    realized: float,
    unrealized: float,
):
    msg = (
        f"ðŸ”” {symbol} {side} {qty:g} @ {fill_price:,.2f}\n"
        f"R: {realized:,.2f} | U: {unrealized:,.2f}"
    )
    send_telegram(msg, silent=False)


def send_drawdown_alert(
    drawdown_pct: float, threshold_pct: float, peak_equity: float, equity: float
):
    global _last_dd_ts
    now = time.time()
    # rateâ€‘limit to once per 15 minutes
    if _last_dd_ts and (now - _last_dd_ts) < 15 * 60:
        return
    _last_dd_ts = now
    msg = (
        f"âš ï¸ Drawdown Alert\n"
        f"DD: {drawdown_pct * 100:.2f}% (thr={threshold_pct * 100:.2f}%)\n"
        f"Equity: {equity:,.2f} | Peak: {peak_equity:,.2f}"
    )
    send_telegram(msg, silent=False)


# --- CLI smoke test ---
if __name__ == "__main__":
    ok = send_telegram("ðŸš€ executor/telegram_utils.py smoke: hello.")
    print("send_ok:", ok)

## execution/universe_resolver.py
#!/usr/bin/env python3
from __future__ import annotations

import json
import os
from typing import Any, Dict, List, Optional, Set, Tuple

try:
    from .exchange_utils import get_symbol_filters  # online listing check
except Exception:  # pragma: no cover
    def get_symbol_filters(_s: str) -> Dict[str, Any]:
        raise RuntimeError("exchange access unavailable")


def _path_tiers() -> str:
    return os.getenv("SYMBOL_TIERS_CONFIG", "config/symbol_tiers.json")


def _path_discovery() -> str:
    return os.getenv("DISCOVERY_PATH", "config/discovery.yml")


def _path_settings() -> str:
    return os.getenv("SETTINGS_PATH", "config/settings.json")


def _load_json(path: str, default):
    try:
        with open(path, "r") as f:
            return json.load(f)
    except Exception:
        return default


def _load_settings() -> Dict[str, Any]:
    return _load_json(_path_settings(), {})


def load_tiers() -> Dict[str, List[str]]:
    tiers = _load_json(_path_tiers(), {})
    out: Dict[str, List[str]] = {}
    for k, v in (tiers.items() if isinstance(tiers, dict) else []):
        if not isinstance(v, list):
            continue
        out[str(k)] = [str(s).upper() for s in v]
    return out


def symbol_tier(symbol: str) -> Optional[str]:
    s = str(symbol).upper()
    tiers = load_tiers()
    for t, arr in tiers.items():
        if s in arr:
            return t
    return None


def is_listed_on_futures(symbol: str) -> bool:
    try:
        f = get_symbol_filters(str(symbol).upper())
        return bool(f)
    except Exception:
        return False


def _load_discovery() -> List[Dict[str, Any]]:
    # Try YAML first; fall back to a minimal ad-hoc parser to avoid dependency
    try:
        import yaml

        with open(_path_discovery(), "r") as f:
            data = yaml.safe_load(f) or []
        return data if isinstance(data, list) else []
    except Exception:
        pass
    # Minimal parser: expect lines like '- key: value' grouped per item
    rows: List[Dict[str, Any]] = []
    cur: Dict[str, Any] = {}
    try:
        with open(_path_discovery(), "r") as f:
            for raw in f:
                ln = raw.strip()
                if not ln:
                    continue
                if ln.startswith("- "):
                    # New item starts
                    if cur:
                        rows.append(cur)
                        cur = {}
                    ln = ln[2:].strip()
                if ":" in ln:
                    k, v = ln.split(":", 1)
                    k = k.strip()
                    v = v.strip()
                    # cast booleans
                    if v.lower() in ("true", "false"):
                        val: Any = v.lower() == "true"
                    else:
                        # strip quotes if any
                        val = v.strip("\"'")
                    cur[k] = val
            if cur:
                rows.append(cur)
    except Exception:
        return []
    return rows


def resolve_allowed_symbols() -> Tuple[List[str], Dict[str, str]]:
    """Return (allowed_symbols, tier_by_symbol).

    allowed = (tiers_whitelist âˆ© exchange_listed âˆ© not_throttled)
    Optionally merge discovery symbols if settings.automerge_discovery is true and records have
    liquidity_ok and trend_ok.
    """
    tiers = load_tiers()
    ordered: List[str] = []
    for key in ("CORE", "SATELLITE", "TACTICAL", "ALT-EXT"):
        ordered.extend([s for s in tiers.get(key, [])])
    tier_by: Dict[str, str] = {}
    for t, arr in tiers.items():
        for s in arr:
            tier_by[s] = t

    # Optional throttle blocklist
    throttle_block: Set[str] = set()
    try:
        cfg = _load_json("config/risk_limits.json", {})
        bl = ((cfg.get("global") or {}).get("throttle") or {}).get("blocked") or []
        throttle_block = {str(x).upper() for x in bl}
    except Exception:
        pass

    # Discovery merge (operator gated)
    settings = _load_settings()
    do_merge = bool((settings.get("settings") or {}).get("automerge_discovery", False)) or bool(
        settings.get("automerge_discovery", False)
    )
    if do_merge:
        for rec in _load_discovery():
            try:
                s = str(rec.get("symbol")).upper()
                if not s:
                    continue
                if not rec.get("liquidity_ok") or not rec.get("trend_ok"):
                    continue
                if s not in ordered:
                    ordered.append(s)
                    tier_by.setdefault(s, "DISCOVERY")
            except Exception:

## execution/utils.py
import json
import os
from datetime import datetime, timezone


def load_env_var(key, default=None):
    val = os.getenv(key)
    if val is None:
        print(f"âš ï¸ Environment variable {key} not set.")
    return val or default


def load_json(path):
    if not os.path.exists(path):
        return {}
    with open(path, "r") as f:
        return json.load(f)


def save_json(path, data):
    with open(path, "w") as f:
        json.dump(data, f, indent=2)


def log_trade(entry, path="logs/trade_log.json"):
    log = load_json(path)
    timestamp = datetime.now(timezone.utc).isoformat()
    log[timestamp] = entry
    save_json(path, log)


def load_local_state(path="synced_state.json"):
    """Loads local synced state from JSON."""
    return load_json(path)


def write_nav_snapshot(nav_usdt: float, breakdown: dict, path: str = "logs/nav_snapshot.json") -> None:
    try:
        os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
        payload = {
            "nav_usdt": float(nav_usdt),
            "breakdown": breakdown,
            "ts": datetime.now(timezone.utc).isoformat(),
        }
        save_json(path, payload)
    except Exception:
        pass


def write_nav_snapshots_pair(
    trading: tuple[float, dict], reporting: tuple[float, dict]
) -> None:
    try:
        nav_t, det_t = trading
        nav_r, det_r = reporting
        write_nav_snapshot(nav_t, det_t, "logs/nav_trading.json")
        write_nav_snapshot(nav_r, det_r, "logs/nav_reporting.json")
        # Legacy single snapshot (trading NAV)
        write_nav_snapshot(nav_t, det_t, "logs/nav_snapshot.json")
    except Exception:
        pass


def write_treasury_snapshot(
    val_usdt: float, breakdown: dict, path: str = "logs/nav_treasury.json"
) -> None:
    try:
        os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
        payload = {
            "treasury_usdt": float(val_usdt),
            "breakdown": breakdown,
            "ts": datetime.now(timezone.utc).isoformat(),
        }
        save_json(path, payload)
    except Exception:
        pass

## scripts/ai_discovery.py
#!/usr/bin/env python3
from __future__ import annotations

import argparse
import os
import sys
from typing import Any, Dict, List


def _futures_listed(symbol: str) -> bool:
    try:
        from execution.exchange_utils import get_symbol_filters

        _ = get_symbol_filters(symbol)
        return True
    except Exception:
        return False


def _pick_candidates(max_n: int = 5) -> List[str]:
    # Basic placeholder universe; in practice, you might screen by volume/ATR.
    seeds = [
        "ARBUSDT",
        "OPUSDT",
        "AVAXUSDT",
        "APTUSDT",
        "INJUSDT",
        "RNDRUSDT",
        "SEIUSDT",
        "TONUSDT",
        "XRPUSDT",
        "ADAUSDT",
    ]
    out = []
    for s in seeds:
        if _futures_listed(s):
            out.append(s)
        if len(out) >= max_n:
            break
    return out


def _ai_rank(symbols: List[str]) -> List[Dict[str, Any]]:
    key = os.getenv("OPENAI_API_KEY", "")
    out: List[Dict[str, Any]] = []
    if not key:
        # Offline fallback: trivial rationale
        for s in symbols:
            out.append(
                {
                    "symbol": s,
                    "rationale": "Adequate liquidity; trend filter passed (stub).",
                    "liquidity_ok": True,
                    "trend_ok": True,
                }
            )
        return out
    try:
        import openai

        client = openai.OpenAI(api_key=key)
        prompt = (
            "Given these USDT-margined Binance futures symbols: "
            + ", ".join(symbols)
            + ". Pick up to 5 with adequate liquidity and a mild positive trend."
            "Return JSON array with objects: {symbol, rationale, liquidity_ok, trend_ok}."
        )
        resp = client.chat.completions.create(
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            max_tokens=300,
        )
        # Be defensive parsing
        import json

        txt = (resp.choices[0].message.content or "[]").strip()
        arr = json.loads(txt)
        if isinstance(arr, list):
            for rec in arr:
                try:
                    s = str(rec.get("symbol")).upper()
                    out.append(
                        {
                            "symbol": s,
                            "rationale": str(rec.get("rationale") or "n/a"),
                            "liquidity_ok": bool(rec.get("liquidity_ok", True)),
                            "trend_ok": bool(rec.get("trend_ok", True)),
                        }
                    )
                except Exception:
                    continue
    except Exception:
        # Fall back to offline stub
        out = _ai_rank(symbols=[])
    return out[:5]


def save_yaml(rows: List[Dict[str, Any]], path: str = "config/discovery.yml") -> str:
    try:
        import yaml  # type: ignore

        with open(path, "w") as f:
            yaml.safe_dump(rows, f, sort_keys=False)
        return path
    except Exception as e:
        raise RuntimeError(f"yaml write error: {e}")


def main(argv: List[str] | None = None) -> int:
    ap = argparse.ArgumentParser(description="AI discovery list generator (operator-gated)")
    ap.add_argument("--max", type=int, default=5)
    args = ap.parse_args(argv)
    cands = _pick_candidates(max_n=int(args.max))
    rows = _ai_rank(cands)
    path = save_yaml(rows)
    print({"ok": True, "path": path, "count": len(rows)})
    return 0


if __name__ == "__main__":
    sys.exit(main())


## scripts/balance_doctor.py
#!/usr/bin/env python
import json, time
from execution.exchange_utils import _client, _futures_base_url  # adjust if wrapped
from execution.exchange_utils import get_balances  # your high-level
def main():
    c = _client()
    # Raw futures: /fapi/v2/balance and /fapi/v2/account
    rb = c.futures_account_balance()
    ra = c.futures_account()  # positions and wallet
    print("[raw futures balance]")
    print(json.dumps(rb, indent=2)[:2000])
    print("[raw futures account keys]", list(ra.keys()))
    print("[computed get_balances()]")
    b = get_balances()
    print(json.dumps(b, indent=2))
if __name__ == "__main__":
    main()

## scripts/binance_auth_doctor.py
import hashlib
import hmac
import os
import sys
import time

import requests

BASE = "https://fapi.binance.com"  # USD-M mainnet
KEY = os.environ.get("BINANCE_API_KEY", "").strip()
SEC = os.environ.get("BINANCE_API_SECRET", "").strip().encode()


def _signed_get(path: str, params=None, recv_window: int = 5000) -> requests.Response:
    params = dict(params or {})
    params["timestamp"] = int(time.time() * 1000)
    params["recvWindow"] = recv_window
    qs = "&".join(f"{k}={params[k]}" for k in sorted(params))
    sig = hmac.new(SEC, qs.encode(), hashlib.sha256).hexdigest()
    url = f"{BASE}{path}?{qs}&signature={sig}"
    return requests.get(url, headers={"X-MBX-APIKEY": KEY}, timeout=10)


def main() -> None:
    print("[doctor] base:", BASE, "key_len:", len(KEY), "secret_len:", len(SEC))
    if len(KEY) == 0 or len(SEC) < 32:
        print(
            "AUTH_DOCTOR_FAIL: Missing/short Binance API credentials in environment (.env not loaded?).",
            file=sys.stderr,
        )
        print(
            "Action: set BINANCE_API_KEY / BINANCE_API_SECRET in .env and 'set -a; source ./.env; set +a'",
            file=sys.stderr,
        )
        sys.exit(2)
    r_pub = requests.get(f"{BASE}/fapi/v1/exchangeInfo", timeout=10)
    print("[doctor] exchangeInfo:", r_pub.status_code)

    # Use v2 account endpoint (v1 may 404 HTML under proxies)
    r1 = _signed_get("/fapi/v2/account")
    print("[doctor] /fapi/v2/account:", r1.status_code)

    r2 = _signed_get("/fapi/v2/positionRisk")
    print("[doctor] /fapi/v2/positionRisk:", r2.status_code)

    bad = (r1.status_code in (401, 403)) or (r2.status_code in (401, 403))
    if bad:
        print("\n[doctor] HINTS:")
        print("- Enable USD-M Futures permission on this API key")
        print("- If IP-restricted, add the server public IPv4 to the whitelist")
        print("- If permissions/whitelist were changed, REGENERATE the secret and update .env")
        print("- Ensure key belongs to the correct (sub)account USD-M wallet")
        print("- Clock skew unlikely if NTP is synchronized")


if __name__ == "__main__":
    main()

## scripts/doctor.py
#!/usr/bin/env python3
import os
import json
import time
import hmac
import hashlib
from typing import Any, Dict
import requests

# --- Load screener pieces
import execution.signal_screener as sc

def _now_ms(): return int(time.time()*1000)

def _dual_side():
    if os.getenv("USE_FUTURES","") not in ("1","true","True"):
        return False, "spot-mode"
    key = os.environ.get("BINANCE_API_KEY","")
    sec = os.environ.get("BINANCE_API_SECRET","").encode()
    base = "https://testnet.binancefuture.com" if os.getenv("BINANCE_TESTNET","") in ("1","true","True") else "https://fapi.binance.com"
    q = f"timestamp={_now_ms()}"
    sig = hmac.new(sec, q.encode(), hashlib.sha256).hexdigest()
    r = requests.get(f"{base}/fapi/v1/positionSide/dual?{q}&signature={sig}", headers={"X-MBX-APIKEY":key}, timeout=10)
    j = r.json()
    return bool(j.get("dualSidePosition", False)), j

def _price(sym:str):
    try:
        from execution.exchange_utils import get_price
        p = get_price(sym)
        return None if (p is None or (isinstance(p,(int,float)) and p<=0)) else float(p)
    except Exception: return None

def _series(st:dict, sym:str, tf:str):
    arr = sc._series_for(st, sym, tf)
    return [p for _,p in arr]

def _meta(st:dict, sym:str, tf:str):
    return ((st.get(sym, {}) or {}).get(f"{tf}__meta", {}) or {})

def main():
    cfg = sc._load_cfg()
    st  = sc._safe_load_json(sc.STATE_PATH, {})
    out = {"env":{}, "strategies":[]}

    # env snapshot
    out["env"]["dualSide"], out["env"]["dualSide_raw"] = _dual_side()
    out["env"]["telegram_enabled"] = os.getenv("TELEGRAM_ENABLED","") in ("1","true","True")
    out["env"]["heartbeat_minutes"] = os.getenv("HEARTBEAT_MINUTES","")
    out["env"]["dry_run"] = os.getenv("DRY_RUN","")
    out["env"]["testnet"] = os.getenv("BINANCE_TESTNET","")
    out["env"]["use_futures"] = os.getenv("USE_FUTURES","")

    for name, scfg in (cfg.get("strategies") or {}).items():
        sym = scfg["symbol"]
        tf  = scfg.get("timeframe","30m")
        pxs = _series(st, sym, tf)
        meta= _meta(st, sym, tf)
        prev_z = meta.get("prev_z", None)
        in_trade = bool(meta.get("in_trade", False))
        side = meta.get("side")
        px_live = _price(sym)

        inds: Dict[str, Any] = {}
        if pxs:
            inds = sc._fetch_indicators_from_your_stack(sym, tf, scfg) or sc._compute_indicators_from_series(pxs, scfg)
        z   = float(inds.get("z", 0.0))
        rsi = float(inds.get("rsi", 50.0))
        atr = float(inds.get("atr_proxy", 0.0))

        entry = scfg.get("entry",{})
        zmin  = float(entry.get("zscore_min", 0.8))
        rband = entry.get("rsi_band", [0,100])
        rlo, rhi = float(rband[0]), float(rband[1])
        atr_min = float(entry.get("atr_min", 0.0))
        rsi_ok  = (rlo <= rsi <= rhi)
        atr_ok  = atr >= atr_min

        if prev_z is None:
            cross_up = cross_down = False
            cross_reason = "prev_z not seeded"
        else:
            cross_up   = (prev_z <  zmin) and (z >=  zmin)
            cross_down = (prev_z > -zmin) and (z <= -zmin)
            cross_reason = "cross ok" if (cross_up or cross_down) else "no cross"

        blocked_by = []
        if px_live is None: blocked_by.append("price_unavailable")
        if not (cross_up or cross_down): blocked_by.append("no_cross")
        if not rsi_ok: blocked_by.append("rsi_veto")
        if not atr_ok: blocked_by.append("atr_floor")
        if in_trade:   blocked_by.append("already_in_trade")

        out["strategies"].append({
            "name": name, "symbol": sym, "tf": tf,
            "series_len": len(pxs),
            "live_px": px_live,
            "z": z, "prev_z": prev_z,
            "cross_up": cross_up, "cross_down": cross_down, "cross_reason": cross_reason,
            "rsi": rsi, "rsi_band": [rlo, rhi], "rsi_ok": rsi_ok,
            "atr": atr, "atr_min": atr_min, "atr_ok": atr_ok,
            "in_trade": in_trade, "side": side,
            "blocked_by": blocked_by
        })

    print(json.dumps(out, indent=2, sort_keys=False))

if __name__ == "__main__":
    main()

## scripts/examples/grep_patterns.txt
[screener]
[screener->executor]
[decision]

## scripts/exec_once_timeout.sh
#!/usr/bin/env bash
set -euo pipefail
cd "$(dirname "$0")/.."

# Hard time limit for a single ONE_SHOT run (default 45s)
MAX_SEC="${EXECUTOR_MAX_SEC:-45}"

set -a
source ./.env
set +a
export PYTHONPATH="$(pwd)"
export FIRESTORE_ENABLED=0
export ONE_SHOT=1
export PYTHONUNBUFFERED=1

# Use coreutils timeout to avoid hangs; if unavailable, fallback to python -c alarm
if command -v timeout >/dev/null 2>&1; then
  timeout -s SIGINT "${MAX_SEC}s" ./venv/bin/python -m execution.executor_live
else
  python3 - <<'PY'
import os, signal, sys, time, threading, subprocess
max_sec = int(os.environ.get("EXECUTOR_MAX_SEC","45"))
def killer(p):
    time.sleep(max_sec)
    try:
        p.send_signal(signal.SIGINT)
    except Exception:
        pass
cmd = [os.path.join("venv","bin","python"), "-m", "execution.executor_live"]
p = subprocess.Popen(cmd, env=os.environ.copy())
threading.Thread(target=killer, args=(p,), daemon=True).start()
p.wait()
sys.exit(p.returncode)
PY
fi

## scripts/fetch_synced_state.py
# scripts/fetch_synced_state.py

import os
import json
from pathlib import Path
from firebase_admin import credentials, firestore, initialize_app

FIREBASE_CREDS = os.getenv("FIREBASE_CREDS_PATH", "config/firebase_creds.json")
LOCAL_STATE_PATH = "synced_state.json"
FIRESTORE_COLLECTION = "hedge_fund"
FIRESTORE_DOCUMENT = "synced_state"

def init_firebase():
    if not Path(FIREBASE_CREDS).exists():
        raise FileNotFoundError(f"Firebase credentials not found at {FIREBASE_CREDS}")
    cred = credentials.Certificate(FIREBASE_CREDS)
    initialize_app(cred)
    return firestore.client()

def fetch_synced_state():
    db = init_firebase()
    doc_ref = db.collection(FIRESTORE_COLLECTION).document(FIRESTORE_DOCUMENT)
    doc = doc_ref.get()

    if not doc.exists:
        raise ValueError("âŒ Firestore document does not exist.")
    
    state = doc.to_dict()
    with open(LOCAL_STATE_PATH, "w") as f:
        json.dump(state, f, indent=2)

    print(f"âœ… Synced Firestore â†’ {LOCAL_STATE_PATH}")
    return state

if __name__ == "__main__":
    try:
        fetch_synced_state()
    except Exception as e:
        print(f"âŒ Error during sync: {e}")

## scripts/fs_doctor.py
#!/usr/bin/env python3
from __future__ import annotations

import argparse
import os
import sys
import time
from typing import Any, Dict, List, Tuple

try:
    from utils.firestore_client import get_db  # type: ignore
except Exception:
    get_db = None  # type: ignore


def _paths(db, env: str) -> Dict[str, Any]:
    root = db.collection("hedge").document(env)
    state = root.collection("state")
    return {
        "nav": state.document("nav"),
        "positions": state.document("positions"),
        "trades": root.collection("trades"),
        "risk": root.collection("risk"),
    }


def _to_epoch(v: Any) -> float:
    try:
        if isinstance(v, (int, float)):
            x = float(v)
            return x / 1000.0 if x > 1e12 else x
        # Firestore timestamp has .timestamp()
        if hasattr(v, "timestamp"):
            return float(v.timestamp())
    except Exception:
        pass
    return 0.0


def run(env: str) -> Tuple[bool, Dict[str, Any]]:
    if get_db is None:
        return False, {"error": "firestore libs unavailable"}
    db = get_db()
    p = _paths(db, env)
    out: Dict[str, Any] = {}

    # NAV
    nav_doc = p["nav"].get()
    nav_series = []
    if getattr(nav_doc, "exists", False):
        data = nav_doc.to_dict() or {}
        for _, v in data.items():
            if not isinstance(v, dict):
                continue
            ts = _to_epoch(v.get("t") or v.get("time") or v.get("ts"))
            nav_series.append(ts)
    out["nav_count"] = len(nav_series)
    out["nav_newest_ts"] = max(nav_series) if nav_series else None

    # Positions
    pos_doc = p["positions"].get()
    pos_rows = 0
    if getattr(pos_doc, "exists", False):
        d = pos_doc.to_dict() or {}
        if isinstance(d.get("rows"), list):
            pos_rows = len(d.get("rows", []))
    out["positions_count"] = pos_rows

    # Trades (24h)
    now = time.time()
    tr_rows: List[float] = []
    docs = list(p["trades"].order_by("ts", direction="DESCENDING").limit(1000).stream())
    env_vals = set()
    tn_vals = set()
    for d in docs:
        x = d.to_dict() or {}
        t = _to_epoch(x.get("ts") or x.get("time") or x.get("t"))
        if (now - t) <= 24 * 3600:
            tr_rows.append(t)
        if "env" in x:
            env_vals.add(str(x.get("env")))
        if "testnet" in x:
            tn_vals.add(bool(x.get("testnet")))
    out["trades_24h"] = len(tr_rows)

    # Risk (24h)
    rk_rows: List[float] = []
    docs_r = list(p["risk"].order_by("ts", direction="DESCENDING").limit(1000).stream())
    for d in docs_r:
        x = d.to_dict() or {}
        t = _to_epoch(x.get("ts") or x.get("time") or x.get("t"))
        if (now - t) <= 24 * 3600:
            rk_rows.append(t)
        if "env" in x:
            env_vals.add(str(x.get("env")))
        if "testnet" in x:
            tn_vals.add(bool(x.get("testnet")))
    out["risk_24h"] = len(rk_rows)

    # Mixing detection
    mixed = (len(env_vals) > 1) or (len(tn_vals) > 1)
    out["mixed"] = bool(mixed)

    # If Telegram enabled, require collections exist and have at least one 24h doc
    tel_enabled = str(os.getenv("TELEGRAM_ENABLED", "0")).lower() in ("1", "true", "yes")
    collections_ok = True
    if tel_enabled:
        try:
            _ = list(p["trades"].limit(1).stream())
            _ = list(p["risk"].limit(1).stream())
        except Exception:
            collections_ok = False
        if out["trades_24h"] == 0 or out["risk_24h"] == 0:
            collections_ok = False
    out["collections_ok"] = collections_ok

    return (not mixed) and collections_ok, out


def main(argv: List[str] | None = None) -> int:
    ap = argparse.ArgumentParser(description="Firestore doctor: counts and mixing detector")
    ap.add_argument("--env", default=os.getenv("ENV", "prod"))
    args = ap.parse_args(argv)
    ok, info = run(args.env)
    print({"ok": ok, **info})
    # Exit non-zero if mixing detected or collections missing when TELEGRAM enabled (prod only)
    if not ok and str(args.env) == "prod":
        return 2
    return 0


if __name__ == "__main__":
    sys.exit(main())

## scripts/go_live_now.sh
#!/usr/bin/env bash
set -euo pipefail
cd "$(dirname "$0")/.."

# Export environment from .env without leaking secrets
set -a
source ./.env
set +a
export PYTHONPATH="$(pwd)"
export BINANCE_TESTNET="${BINANCE_TESTNET:-0}"
export ENV="${ENV:-prod}"
export DRY_RUN="${DRY_RUN:-0}"
export EVENT_GUARD="${EVENT_GUARD:-1}"
export FIRESTORE_ENABLED="${FIRESTORE_ENABLED:-0}"
export PYTHONUNBUFFERED=1

# Basic connectivity and auth diagnostics
echo "== Auth Doctor =="
./venv/bin/python scripts/binance_auth_doctor.py || true

# Optional margin mode helper
if [[ -x "scripts/margin_mode_once.py" ]]; then
  echo "== Margin mode (CROSS) =="
  BINANCE_TESTNET=0 ./venv/bin/python scripts/margin_mode_once.py || true
fi

# One-shot warmup with timeout safeguard
if [[ -x "scripts/exec_once_timeout.sh" ]]; then
  echo "== Warmup ONE_SHOT with timeout =="
  EXECUTOR_MAX_SEC=45 bash scripts/exec_once_timeout.sh || true
fi

# Continuous executor with log rollover
mkdir -p logs
LOGFILE="logs/executor_live.$(date -u +%Y%m%dT%H%M%SZ).log"
echo "== Starting continuous live executor (logging to $LOGFILE) =="
while true; do
  date -u +"[start %FT%TZ]" | tee -a "$LOGFILE"
  ONE_SHOT=0 ./venv/bin/python -m execution.executor_live 2>&1 | tee -a "$LOGFILE"
  code=$?
  date -u +"[exit %FT%TZ rc=$code]" | tee -a "$LOGFILE"
  sleep 3
done

## scripts/grep_order_req.sh
#!/usr/bin/env bash
set -euo pipefail
cd "$(dirname "$0")/.."

LOG1="deploy/supervisor-user/logs/hedge-executor.out.log"
LOG2="/var/log/hedge-executor.out.log"

log=""
if [[ -f "$LOG1" ]]; then log="$LOG1"; fi
if [[ -z "$log" && -f "$LOG2" ]]; then log="$LOG2"; fi
if [[ -z "$log" ]]; then
  echo "No known executor log file found."
  exit 2
fi

echo "Scanning: $log"
# Count successful order requests and show last few
grep -E 'ORDER_REQ.* 200' -n "$log" | tail -n 20 || true
COUNT=$(grep -E 'ORDER_REQ.* 200' -c "$log" || true)
echo "ORDER_REQ_200_COUNT=$COUNT"

# If zero, surface top veto reasons (if present)
if [[ "${COUNT:-0}" -eq 0 ]]; then
  echo "No ORDER_REQ 200 found. Top veto markers (last 200 lines):"
  tail -n 200 "$log" | grep -E 'veto|portfolio_cap|symbol_cap|tier_cap|cooldown|daily_loss|below_min_notional|trade_rate_limit|ob_adverse' || true
fi

## scripts/leverage_once.py
#!/usr/bin/env python3
import os
import time
import hmac
import hashlib
import requests
import argparse
B="https://testnet.binancefuture.com" if str(os.getenv("BINANCE_TESTNET","1")).lower() in ("1","true","yes","on") else "https://fapi.binance.com"
AK=os.environ["BINANCE_API_KEY"]; SK=os.environ["BINANCE_API_SECRET"]; H={"X-MBX-APIKEY":AK}
def sig(q): return hmac.new(SK.encode(), q.encode(), hashlib.sha256).hexdigest()
def set_leverage(sym: str, lev: int):
    ts=str(int(time.time()*1000)); q=f"symbol={sym}&leverage={lev}&timestamp={ts}&recvWindow=5000"
    r=requests.post(B+"/fapi/v1/leverage?"+q+"&signature="+sig(q), headers=H, timeout=10)
    print(sym, r.status_code, r.text); r.raise_for_status()
if __name__=="__main__":
    ap=argparse.ArgumentParser(); ap.add_argument("--lev", type=int, required=True); ap.add_argument("symbols", nargs="+")
    a=ap.parse_args(); [set_leverage(s, a.lev) for s in a.symbols]

## scripts/margin_mode_once.py
#!/usr/bin/env python3
"""
Set USD-M futures marginType=CROSSED once per symbol (idempotent).
Skips with code -4046 "No need to change margin type."
"""
import os
import time
import hmac
import hashlib
import requests
import argparse

B = "https://testnet.binancefuture.com" if str(os.getenv("BINANCE_TESTNET","1")).lower() in ("1","true","yes","on") else "https://fapi.binance.com"
AK = os.environ["BINANCE_API_KEY"]; SK = os.environ["BINANCE_API_SECRET"]
H  = {"X-MBX-APIKEY": AK}

def sig(q): return hmac.new(SK.encode(), q.encode(), hashlib.sha256).hexdigest()

def set_cross(sym: str):
    ts = str(int(time.time()*1000))
    q  = f"symbol={sym}&marginType=CROSSED&timestamp={ts}&recvWindow=5000"
    r  = requests.post(B+"/fapi/v1/marginType?"+q+"&signature="+sig(q), headers=H, timeout=10)
    if r.status_code == 200:
        print(sym, "OK", r.text)
    else:
        t = r.text
        if '"code":-4046' in t:
            print(sym, "SKIP already CROSSED")
        else:
            print(sym, "ERR", t)
            r.raise_for_status()

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("symbols", nargs="+", help="Symbols to set marginType=CROSSED")
    a = ap.parse_args()
    for s in a.symbols:
        set_cross(s)

## scripts/ml_fit.py
#!/usr/bin/env python3
import json
from execution.utils import load_json
from execution.ml.train import train_all


def _enable_simulator_if_requested():
    import os

    if os.environ.get("ML_SIMULATOR", "0") != "1":
        return

    import numpy as np
    import pandas as pd
    from execution.ml import data as data_mod

    def _fake_klines(symbol, interval, limit):
        idx = pd.date_range("2024-01-01", periods=limit, freq="H", tz="UTC")
        drift = np.linspace(0, 0.005 * limit, limit)
        noise = np.random.normal(0, 0.2, size=limit).cumsum()
        base = 100 + drift + noise
        df = pd.DataFrame(
            {
                "open": base,
                "high": base * 1.001,
                "low": base * 0.999,
                "close": base,
                "volume": 1.0,
            },
            index=idx,
        )
        rows = []
        for ts, row in df.iterrows():
            rows.append(
                [
                    int(ts.value // 10**6),
                    float(row["open"]),
                    float(row["high"]),
                    float(row["low"]),
                    float(row["close"]),
                    float(row["volume"]),
                ]
            )
        data_mod.get_klines = _fake_klines  # type: ignore[attr-defined]


def main() -> None:
    _enable_simulator_if_requested()
    cfg = load_json("config/strategy_config.json")
    metas = train_all(cfg)
    print(json.dumps(metas, indent=2))


if __name__ == "__main__":
    main()

## scripts/ml_health.sh
#!/usr/bin/env bash
set -euo pipefail
cd "$(dirname "$0")/.."

echo "== ML Health =="
for f in models/registry.json models/signal_eval.json models/last_train_report.json; do
  if [[ -f "$f" ]]; then
    printf "[OK] %s (updated: %s)\n" "$f" "$(date -u -r "$f" +%FT%TZ)"
  else
    echo "[MISS] $f"
  fi
done

echo "---- last_train_report.json (tail) ----"
if [[ -f models/last_train_report.json ]]; then
  tail -n 50 models/last_train_report.json
fi

## scripts/ml_retrain_cron.sh
#!/usr/bin/env bash
# Nightly retrain + evaluation for cron (preferred path, no supervisor needed).
# Example crontab entry (UTC):
#   10 2 * * * cd /path/to/hedge-fund && /bin/bash scripts/ml_retrain_cron.sh >> models/cron.log 2>&1
set -euo pipefail
cd "$(dirname "$0")/.."
set -a
source ./.env 2>/dev/null || true
set +a
export PYTHONPATH="$(pwd)"
mkdir -p models

ts() { date -u +%FT%TZ; }

START_TS="$(ts)"
FIT_RC=0
EVAL_RC=0

FIT_OUT="$('./venv/bin/python' scripts/ml_fit.py 2>&1)" || FIT_RC=$?
EVAL_OUT="$('./venv/bin/python' scripts/signal_eval.py 2>&1)" || EVAL_RC=$?

python3 - "$START_TS" "$FIT_RC" "$EVAL_RC" "$FIT_OUT" "$EVAL_OUT" > models/last_train_report.json <<'PY'
from datetime import datetime, timezone
import json, sys
start_ts, fit_rc, eval_rc, fit_out, eval_out = sys.argv[1], int(sys.argv[2]), int(sys.argv[3]), sys.argv[4], sys.argv[5]
def parse_raw(raw: str):
    try:
        return json.loads(raw)
    except Exception:
        return {"stdout_tail": raw[-2000:]}
report = {
    "started_at_utc": start_ts,
    "finished_at_utc": datetime.now(timezone.utc).isoformat(),
    "fit_rc": fit_rc,
    "fit_result": parse_raw(fit_out),
    "eval_rc": eval_rc,
    "eval_result": parse_raw(eval_out),
}
print(json.dumps(report, indent=2))
PY

echo "[cron] retrain+eval done at $(ts) (fit_rc=$FIT_RC, eval_rc=$EVAL_RC)"

## scripts/ml_retrain_daemon.py
#!/usr/bin/env python3
"""
Nightly retrain + evaluation daemon (no sudo). Sleeps until next 02:10 UTC,
then runs:
  - scripts/ml_fit.py
  - scripts/signal_eval.py
Also writes models/last_train_report.json (summary).
Exit only on fatal error; otherwise loop forever.
"""
import datetime as dt
import json
import os
import subprocess
import sys
import time
from pathlib import Path

REPO = Path(__file__).resolve().parents[1]
MODELS = REPO / "models"
MODELS.mkdir(exist_ok=True)


def utc_now() -> dt.datetime:
    return dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc)


def seconds_until(hour: int = 2, minute: int = 10, second: int = 0) -> float:
    now = utc_now()
    target = now.replace(hour=hour, minute=minute, second=second, microsecond=0)
    if target <= now:
        target += dt.timedelta(days=1)
    return (target - now).total_seconds()


def run_cmd(cmd, env=None, timeout=3600):
    proc = subprocess.run(
        cmd,
        capture_output=True,
        text=True,
        timeout=timeout,
        env=env or os.environ.copy(),
    )
    return proc.returncode, proc.stdout, proc.stderr


def main() -> None:
    os.chdir(REPO)
    while True:
        wait_seconds = int(seconds_until())
        print(f"[ml-retrain] sleeping {wait_seconds}s until next 02:10 UTC", flush=True)
        try:
            time.sleep(wait_seconds)
        except KeyboardInterrupt:
            print("[ml-retrain] interrupted; exiting", flush=True)
            return

        iteration = {
            "started_at_utc": utc_now().isoformat(),
        }
        t0 = time.time()

        rc_fit, out_fit, err_fit = run_cmd([sys.executable, "scripts/ml_fit.py"], timeout=5400)
        iteration["fit_rc"] = rc_fit
        try:
            iteration["fit_result"] = json.loads(out_fit)
        except Exception:
            iteration["fit_stdout_tail"] = out_fit[-2000:]
        if err_fit:
            iteration["fit_stderr_tail"] = err_fit[-2000:]

        rc_eval, out_eval, err_eval = run_cmd([sys.executable, "scripts/signal_eval.py"], timeout=1800)
        iteration["eval_rc"] = rc_eval
        try:
            iteration["eval_result"] = json.loads(out_eval)
        except Exception:
            iteration["eval_stdout_tail"] = out_eval[-2000:]
        if err_eval:
            iteration["eval_stderr_tail"] = err_eval[-2000:]

        iteration["finished_at_utc"] = utc_now().isoformat()
        iteration["elapsed_sec"] = round(time.time() - t0, 2)

        with open(MODELS / "last_train_report.json", "w", encoding="utf-8") as handle:
            json.dump(iteration, handle, indent=2)

        print("[ml-retrain] cycle complete", flush=True)


if __name__ == "__main__":
    try:
        main()
    except Exception as exc:  # pragma: no cover
        print(f"[ml-retrain] fatal error: {exc}", file=sys.stderr)
        raise

## scripts/ml_retrain_now.sh
#!/usr/bin/env bash
set -euo pipefail
cd "$(dirname "$0")/.."
exec /bin/bash scripts/ml_retrain_cron.sh

## scripts/ml_score.py
#!/usr/bin/env python3
import json
from execution.utils import load_json
from execution.ml.predict import score_all


def main() -> None:
    cfg = load_json("config/strategy_config.json")
    results = score_all(cfg)
    print(json.dumps(results, indent=2))


if __name__ == "__main__":
    main()

## scripts/portfolio_probe.py
#!/usr/bin/env python3
import json
import os

from execution.utils import load_json
from execution.nav import PortfolioSnapshot


def main() -> None:
    cfg = load_json("config/strategy_config.json") or {}
    snapshot = PortfolioSnapshot(cfg)
    nav_usd = float(snapshot.current_nav_usd())
    gross_usd = float(snapshot.current_gross_usd())

    sizing = (cfg.get("sizing") or {})
    cap_pct = float(sizing.get("max_gross_exposure_pct", 0.0) or 0.0)
    if os.environ.get("EVENT_GUARD", "0") == "1":
        cap_pct *= 0.8
    cap_usd = nav_usd * (cap_pct / 100.0) if nav_usd > 0 and cap_pct > 0 else 0.0
    free_usd = max(cap_usd - gross_usd, 0.0) if cap_usd > 0 else 0.0
    gross_pct = (gross_usd / nav_usd * 100.0) if nav_usd > 0 else 0.0

    payload = {
        "nav_usd": nav_usd,
        "gross_usd": gross_usd,
        "gross_pct_of_nav": gross_pct,
        "cap_pct_effective": cap_pct,
        "cap_usd": cap_usd,
        "free_to_deploy_usd": free_usd,
    }
    print(json.dumps(payload, indent=2, sort_keys=True))


if __name__ == "__main__":
    main()

## scripts/quick_checks.sh
#!/usr/bin/env bash
# Idempotent one-liners for routine checks.
set -Eeuo pipefail

ROOT="/root/hedge-fund"
ENV_FILE="$ROOT/.env"

echo "== Supervisor status =="
supervisorctl status || true
echo

echo "== NGINX reachability (expect 401/200 with auth) =="
curl -s -o /dev/null -w "HTTP %{http_code}\n" http://167.235.205.126/ || true
echo

echo "== Executor environment (Firestore/Binance) =="
pid=$(supervisorctl pid hedge-executor 2>/dev/null || true)
if [[ -n "${pid:-}" && "$pid" =~ ^[0-9]+$ ]] && [[ -r /proc/$pid/environ ]]; then
  tr '\0' '\n' < /proc/$pid/environ | egrep 'ENV=|USE_FUTURES|BINANCE_|GOOGLE|FIREBASE|PYTHONPATH' | sort || true
else
  echo "executor pid not found"
fi
echo

echo "== Hedge vs One-way (dualSide) =="
PYTHONPATH="$ROOT" "$ROOT/venv/bin/python" - <<'PY' || true
from execution.exchange_utils import _is_dual_side
print("dualSide:", _is_dual_side())
PY
echo

echo "== Klines sanity (BTCUSDT 15m last 3 closes) =="
PYTHONPATH="$ROOT" "$ROOT/venv/bin/python" - <<'PY' || true
from execution.exchange_utils import get_klines
k = get_klines("BTCUSDT","15m",limit=3)
print(k)
PY
echo

echo "== Log tail (pipeline) =="
grep -E '\[screener\]|\[decision\]|\[screener->executor\]|\[executor\]' /var/log/hedge-executor.out.log | tail -n 40 || true

## scripts/quick_watch.sh
#!/usr/bin/env bash
set -euo pipefail
cd "$(dirname "$0")/.."

LOG1="deploy/supervisor-user/logs/hedge-executor.out.log"
LOG2="/var/log/hedge-executor.out.log"
LOG3=$(ls -1t logs/executor_live.*.log 2>/dev/null | head -n1 || true)
log=""
for candidate in "$LOG3" "$LOG1" "$LOG2"; do
  if [[ -f "$candidate" ]]; then
    log="$candidate"
    break
  fi
done

if [[ -z "$log" ]]; then
  echo "No log found"
  exit 1
fi

echo "Tailing $log"
tail -n 200 -f "$log" | grep -E --line-buffered 'ORDER_REQ|veto|ml_p='

## scripts/replay_logs.py
import argparse
import json
import time
from datetime import datetime
from pathlib import Path

from google.cloud import firestore

# Firestore client
db = firestore.Client()

def replay_logs(log_dir, accelerated=False, delay=0.5):
    log_files = sorted(Path(log_dir).glob("*.jsonl"))
    if not log_files:
        print("No log files found")
        return

    for file in log_files:
        with open(file, "r") as f:
            for line in f:
                try:
                    event = json.loads(line)
                except json.JSONDecodeError:
                    continue

                ts = event.get("timestamp", time.time())
                doc = normalize_event(event)

                # Push into Firestore
                db.collection("hedge_logs").document(str(ts)).set(doc)

                if not accelerated:
                    time.sleep(delay)

                print(f"Injected {doc}")

def normalize_event(event):
    """Map dry-run log schema â†’ Firestore schema expected by dashboard."""
    return {
        "timestamp": event.get("timestamp", time.time()),
        "symbol": event.get("symbol"),
        "side": event.get("side"),
        "qty": event.get("qty"),
        "price": event.get("price"),
        "nav": event.get("nav"),
        "unrealized_pnl": event.get("unrealized_pnl"),
        "realized_pnl": event.get("realized_pnl"),
        "mode": "replay",
    }

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--log-dir", default="logs/dry_run", help="Path to dry-run JSONL logs")
    parser.add_argument("--accelerated", action="store_true", help="Replay fast instead of real-time pacing")
    parser.add_argument("--delay", type=float, default=0.5, help="Seconds between events if accelerated")
    args = parser.parse_args()

    replay_logs(args.log_dir, accelerated=args.accelerated, delay=args.delay)

## scripts/run_executor_once.sh
#!/usr/bin/env bash
set -euo pipefail
cd "$(dirname "$0")/.."
set -a
source ./.env
set +a
export PYTHONPATH="$(pwd)"
ONE_SHOT=1 ./venv/bin/python -m execution.executor_live

## scripts/screener_probe.py
#!/usr/bin/env python3
from __future__ import annotations

import argparse
import json
import os
from typing import Any, Dict, List, Tuple


def _load_risk_cfg() -> Dict[str, Any]:
    path = os.getenv("RISK_LIMITS_CONFIG", "config/risk_limits.json")
    try:
        with open(path, "r") as f:
            return json.load(f)
    except Exception:
        return {}


def probe_symbols(symbols: List[str], notional: float, lev: float, nav: float) -> List[Dict[str, Any]]:
    from execution.signal_screener import would_emit
    from execution.universe_resolver import symbol_tier

    out: List[Dict[str, Any]] = []
    cfg = _load_risk_cfg()
    max_conc = int(float(((cfg.get("global") or {}).get("max_concurrent_positions", 0)) or 0))

    # For a simple probe, assume zero current exposure/positions, focus on gating logic.
    for sym in symbols:
        ok, reasons, extra = would_emit(
            sym,
            side="BUY",
            notional=notional,
            lev=lev,
            nav=nav,
            open_positions_count=0,
            current_gross_notional=0.0,
            current_tier_gross_notional=0.0,
            orderbook_gate=True,
        )
        out.append(
            {
                "symbol": sym,
                "tier": symbol_tier(sym),
                "would_emit": bool(ok),
                "reasons": reasons,
                "ob": extra.get("flag") if isinstance(extra, dict) else None,
                "max_concurrent_positions": max_conc,
            }
        )
    return out


def main(argv: List[str] | None = None) -> int:
    ap = argparse.ArgumentParser(description="Screener probe: print would_emit and veto stack per symbol")
    ap.add_argument("symbols", nargs="*", help="Symbols to probe (e.g., BTCUSDT ETHUSDT)")
    ap.add_argument("--notional", type=float, default=float(os.getenv("PROBE_NOTIONAL", "10")))
    ap.add_argument("--lev", type=float, default=float(os.getenv("PROBE_LEV", "20")))
    ap.add_argument("--nav", type=float, default=float(os.getenv("PROBE_NAV", "1000")))
    args = ap.parse_args(argv)

    syms = [s.upper() for s in (args.symbols or [])]
    if not syms:
        # default to CORE tier leader candidates if tiers config present
        try:
            tiers = json.load(open(os.getenv("SYMBOL_TIERS_CONFIG", "config/symbol_tiers.json")))
            syms = list((tiers.get("CORE") or [])[:5])
        except Exception:
            syms = ["BTCUSDT"]

    rows = probe_symbols(syms, notional=args.notional, lev=args.lev, nav=args.nav)
    for r in rows:
        print(json.dumps(r))
    return 0


if __name__ == "__main__":
    raise SystemExit(main())


## scripts/seed_universe.py
#!/usr/bin/env python3
from __future__ import annotations
"""
Read exchangeInfo and propose additions to config/pairs_universe.json.
Default is read-only: prints symbols (USD-M, quote=USDT, status=TRADING).
Use --write to merge them into the JSON.
"""
import os
import json
import requests

B = "https://testnet.binancefuture.com" if str(os.getenv("BINANCE_TESTNET","1")).lower() in ("1","true","yes","on") else "https://fapi.binance.com"
ROOT = "/root/hedge-fund"
CONF = f"{ROOT}/config/pairs_universe.json"
AK = os.getenv("BINANCE_API_KEY","")
H = {"X-MBX-APIKEY": AK} if AK else {}

def fetch_symbols():
    r = requests.get(B+"/fapi/v1/exchangeInfo", headers=H, timeout=15); r.raise_for_status()
    data = r.json()
    syms = []
    for s in data.get("symbols", []):
        if s.get("contractType") in ("PERPETUAL","CURRENT_QUARTER","NEXT_QUARTER") and s.get("quoteAsset")=="USDT" and s.get("status")=="TRADING":
            syms.append(s["symbol"])
    return sorted(syms)

def load_universe():
    try:
        with open(CONF, "r") as f: return json.load(f)
    except Exception:
        return {"symbols": [], "overrides": {}}

def save_universe(obj):
    tmp = CONF + ".tmp"
    with open(tmp, "w") as f: json.dump(obj, f, indent=2, sort_keys=True)
    os.replace(tmp, CONF)

if __name__ == "__main__":
    import argparse
    ap = argparse.ArgumentParser()
    ap.add_argument("--write", action="store_true", help="Persist merged universe to config/pairs_universe.json")
    ap.add_argument("--limit", type=int, default=50, help="Only show first N additions (for preview)")
    a = ap.parse_args()

    live = set(load_universe().get("symbols", []))
    exch = set(fetch_symbols())
    add  = sorted(exch - live)

    if not add:
        print("Universe up-to-date. No additions.")
    else:
        print("Candidates to add (first {}):".format(a.limit))
        for s in add[:a.limit]:
            print("-", s)

        if a.write:
            u = load_universe()
            u_syms = sorted(set(u.get("symbols", [])) | exch)
            u["symbols"] = u_syms
            for s in add:
                u["overrides"].setdefault(s, {"target_leverage": 3, "min_notional": 5.0})
            save_universe(u)
            print("config/pairs_universe.json updated.")

## scripts/signal_eval.py
#!/usr/bin/env python3
"""
Offline evaluation of signal generation comparing ML probability gate vs rule-based signals.
Outputs models/signal_eval.json summarising per-symbol metrics and aggregate averages.
"""
import json
from datetime import datetime, timezone
from pathlib import Path

import numpy as np
import pandas as pd
from sklearn.metrics import precision_recall_fscore_support, roc_auc_score

from execution.ml.data import load_candles
from execution.ml.features import build_feature_frame, make_labels
from execution.utils import load_json


def _rule_signal(close: pd.Series, sig_cfg: dict) -> pd.Series:
    mom = sig_cfg.get("momentum", {})
    ema_fast = close.ewm(span=mom.get("ema_fast", 20), adjust=False).mean()
    ema_slow = close.ewm(span=mom.get("ema_slow", 50), adjust=False).mean()
    delta = close.diff()
    up = delta.clip(lower=0).ewm(alpha=1 / max(1, mom.get("rsi_len", 14)), adjust=False).mean()
    down = (-delta.clip(upper=0)).ewm(alpha=1 / max(1, mom.get("rsi_len", 14)), adjust=False).mean()
    rs = up / (down.replace(0, np.nan))
    rsi = 100 - (100 / (1 + rs))
    rsi = rsi.fillna(50)
    return ((ema_fast > ema_slow) & (rsi >= mom.get("rsi_buy", 55))).astype(int)


def _metrics(y_true: np.ndarray, signal: np.ndarray, proba: np.ndarray | None = None) -> dict:
    if len(y_true) == 0:
        return {"auc": float("nan"), "precision": 0.0, "recall": 0.0, "f1": 0.0, "hit_rate": 0.0, "coverage": 0.0}
    try:
        precision, recall, f1, _ = precision_recall_fscore_support(
            y_true, signal, average="binary", zero_division=0
        )
    except Exception:
        precision = recall = f1 = 0.0
    hit = float((signal * y_true).sum() / max(1, signal.sum()))
    coverage = float(signal.mean())
    if proba is not None and len(set(y_true)) > 1:
        try:
            auc = float(roc_auc_score(y_true, proba))
        except Exception:
            auc = float("nan")
    else:
        auc = float("nan")
    return {
        "auc": auc,
        "precision": float(precision),
        "recall": float(recall),
        "f1": float(f1),
        "hit_rate": hit,
        "coverage": coverage,
    }


def evaluate_symbol(cfg: dict, symbol: str) -> dict:
    ml_cfg = cfg.get("ml", {})
    interval = ml_cfg.get("timeframe", "1h")
    lookback = int(ml_cfg.get("lookback_bars", 2000))
    horizon = int(ml_cfg.get("horizon_bars", 4))
    target_bps = float(ml_cfg.get("target_ret_bps", 20.0))
    prob_threshold = float(ml_cfg.get("prob_threshold", 0.5))

    try:
        candles = load_candles(symbol, interval, lookback + horizon + 20)
    except Exception as exc:
        return {"symbol": symbol, "error": f"load_candles_failed: {exc}"}

    try:
        feats = build_feature_frame(candles, cfg)
    except Exception as exc:
        return {"symbol": symbol, "error": f"feature_build_failed: {exc}"}

    try:
        labels = make_labels(candles["close"], horizon, target_bps).reindex(feats.index).dropna()
    except Exception as exc:
        return {"symbol": symbol, "error": f"label_make_failed: {exc}"}

    feats = feats.loc[labels.index]
    y = labels.values.astype(int)
    if len(y) == 0 or feats.empty:
        return {"symbol": symbol, "error": "insufficient_samples"}

    try:
        rule_sig = _rule_signal(candles["close"], cfg.get("signals", {})).reindex(labels.index).fillna(0).astype(int)
    except Exception as exc:
        return {"symbol": symbol, "error": f"rule_signal_failed: {exc}"}

    proba = np.zeros_like(y, dtype=float)
    try:
        registry = json.load(open(ml_cfg.get("registry_path", "models/registry.json"), "r", encoding="utf-8"))
        meta = registry.get(symbol)
        if meta:
            blob = Path(meta["model_path"])
            if blob.exists():
                import joblib

                model_pack = joblib.load(blob)
                model = model_pack["model"]
                feature_names = model_pack["features"]
                X = feats[feature_names].values
                proba = model.predict_proba(X)[:, 1]
    except Exception:
        proba = np.zeros_like(y, dtype=float)

    ml_signal = (proba >= prob_threshold).astype(int)

    return {
        "symbol": symbol,
        "n": int(len(y)),
        "ml": _metrics(y, ml_signal, proba),
        "rule": _metrics(y, rule_sig.values, None),
    }


def main() -> None:
    cfg = load_json("config/strategy_config.json")

    import os
    if os.environ.get("ML_SIMULATOR", "0") == "1":
        from execution.ml import data as data_mod

        def _fake_klines(symbol, interval, limit):
            idx = pd.date_range("2024-01-01", periods=limit, freq="H", tz="UTC")
            drift = np.linspace(0, 0.004 * limit, limit)
            noise = np.random.normal(0, 0.25, size=limit).cumsum()
            base = 100 + drift + noise
            df = pd.DataFrame(
                {
                    "open": base,
                    "high": base * 1.001,
                    "low": base * 0.999,
                    "close": base,
                    "volume": 1.0,
                },
                index=idx,
            )
            rows = []
            for ts, row in df.iterrows():
                rows.append(
                    [
                        int(ts.value // 10**6),
                        float(row["open"]),
                        float(row["high"]),
                        float(row["low"]),
                        float(row["close"]),

## scripts/sv_user.sh
#!/usr/bin/env bash
set -euo pipefail

ROOT_DIR=$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)
SUPERVISOR_CONF="$ROOT_DIR/deploy/supervisor-user/supervisord.conf"
LOG_DIR="$ROOT_DIR/deploy/supervisor-user/logs"
mkdir -p "$LOG_DIR"

do_help() {
  cat <<USAGE
Usage: $(basename "$0") <boot|start|stop|restart|status> [program]
  boot               Start user-level supervisord daemon
  start <program>    Start program (default: all)
  stop <program>     Stop program (default: all)
  restart <program>  Restart program (default: all)
  status             Show status
USAGE
}

if [[ $# -lt 1 ]]; then
  do_help
  exit 1
fi

CMD=$1
shift || true
PROG=${1:-all}

case "$CMD" in
  boot)
    supervisord -c "$SUPERVISOR_CONF"
    echo "Hint: if \"status\" fails with connection refused, give port 9002 a second to come up." >&2
    ;;
  start)
    supervisorctl -c "$SUPERVISOR_CONF" start "$PROG"
    ;;
  stop)
    supervisorctl -c "$SUPERVISOR_CONF" stop "$PROG"
    ;;
  restart)
    supervisorctl -c "$SUPERVISOR_CONF" restart "$PROG"
    ;;
  status)
    supervisorctl -c "$SUPERVISOR_CONF" status
    ;;
  *)
    do_help
    exit 1
    ;;
esac

## scripts/tail_exec_log.sh
#!/usr/bin/env bash
set -euo pipefail
cd "$(dirname "$0")/.."
if [[ -f "deploy/supervisor-user/logs/hedge-executor.out.log" ]]; then
  tail -n 200 -f deploy/supervisor-user/logs/hedge-executor.out.log
elif [[ -f "/var/log/hedge-executor.out.log" ]]; then
  tail -n 200 -f /var/log/hedge-executor.out.log
else
  echo "No known executor log file found."
  exit 1
fi

## scripts/trim_to_cap.py
#!/usr/bin/env python3
import json
import os
from decimal import Decimal, ROUND_DOWN
from typing import List, Dict, Any

from execution.utils import load_json
from execution.nav import PortfolioSnapshot
from execution.exchange_utils import (
    get_positions,
    get_price,
    get_symbol_filters,
    place_market_order,
)


def _quantize(qty: float, step: float) -> float:
    if step <= 0:
        return float(qty)
    d_step = Decimal(str(step))
    return float((Decimal(str(qty)) / d_step).to_integral_value(rounding=ROUND_DOWN) * d_step)


def main() -> None:
    cfg = load_json("config/strategy_config.json") or {}
    snapshot = PortfolioSnapshot(cfg)
    nav_usd = float(snapshot.current_nav_usd())
    gross_usd = float(snapshot.current_gross_usd())

    sizing = (cfg.get("sizing") or {})
    cap_pct = float(sizing.get("max_gross_exposure_pct", 0.0) or 0.0)
    if os.environ.get("EVENT_GUARD", "0") == "1":
        cap_pct *= 0.8
    cap_usd = nav_usd * (cap_pct / 100.0) if nav_usd > 0 and cap_pct > 0 else 0.0

    if cap_usd <= 0:
        print(json.dumps({"status": "no_cap", "nav_usd": nav_usd, "cap_usd": cap_usd}, indent=2))
        return

    excess = gross_usd - cap_usd
    if excess <= 0:
        print(
            json.dumps(
                {
                    "status": "within_cap",
                    "nav_usd": nav_usd,
                    "gross_usd": gross_usd,
                    "cap_usd": cap_usd,
                },
                indent=2,
            )
        )
        return

    dry_run = os.environ.get("DRY_RUN", "0").lower() in ("1", "true", "yes", "on")
    remaining = excess
    actions: List[Dict[str, Any]] = []

    positions = list(get_positions() or [])
    # Sort by largest notional first
    enriched: List[Dict[str, Any]] = []
    for p in positions:
        qty = float(p.get("qty", p.get("positionAmt", 0.0)) or 0.0)
        if qty == 0.0:
            continue
        symbol = str(p.get("symbol", "")).upper()
        mark = float(p.get("markPrice") or 0.0)
        if mark <= 0:
            try:
                mark = float(get_price(symbol))
            except Exception:
                mark = abs(float(p.get("entryPrice") or 0.0))
        notional = abs(qty) * abs(mark)
        enriched.append(
            {
                "symbol": symbol,
                "qty": qty,
                "abs_qty": abs(qty),
                "mark": mark,
                "notional": notional,
                "positionSide": p.get("positionSide", "BOTH"),
            }
        )

    enriched.sort(key=lambda x: x["notional"], reverse=True)

    total_trimmed = 0.0
    for pos in enriched:
        if remaining <= 0:
            break
        symbol = pos["symbol"]
        qty = pos["qty"]
        abs_qty = pos["abs_qty"]
        mark = max(pos["mark"], 0.0)
        if mark <= 0:
            continue
        filters = get_symbol_filters(symbol)
        lot = filters.get("MARKET_LOT_SIZE") or filters.get("LOT_SIZE") or {}
        step = float(lot.get("stepSize", 0.0) or 0.0)

        max_reducible_usd = abs_qty * mark
        target_usd = min(remaining, max_reducible_usd)
        if target_usd <= 0:
            continue

        qty_target = min(abs_qty, target_usd / mark)
        qty_rounded = _quantize(qty_target, step)
        if qty_rounded <= 0:
            qty_rounded = _quantize(abs_qty, step)
        if qty_rounded <= 0:
            continue

        order_side = "SELL" if qty > 0 else "BUY"
        position_side = pos.get("positionSide") or ("LONG" if qty > 0 else "SHORT")
        notional_sent = qty_rounded * mark

        action = {
            "symbol": symbol,
            "side": order_side,
            "positionSide": position_side,
            "qty": qty_rounded,
            "approx_notional": notional_sent,
            "reduceOnly": True,
        }

        if dry_run:
            action["status"] = "dry_run"
        else:
            try:
                resp = place_market_order(
                    symbol=symbol,
                    side=order_side,
                    quantity=qty_rounded,
                    position_side=position_side,
                    reduce_only=True,
                )
                action["status"] = "ok"
                action["response"] = resp
            except Exception as exc:
                action["status"] = "error"
                action["error"] = str(exc)
                actions.append(action)
                # If order failed we do not adjust remaining
                continue

        remaining -= notional_sent
        total_trimmed += notional_sent
        actions.append(action)

    summary = {

## scripts/write_keys_env.sh
#!/usr/bin/env bash
# Usage:
#   bash scripts/write_keys_env.sh "<API_KEY>" "<API_SECRET>"
# or (safer for shell history):
#   BINANCE_API_KEY="..." BINANCE_API_SECRET="..." bash scripts/write_keys_env.sh
set -euo pipefail
cd "$(dirname "$0")/.."

KEY="${1:-${BINANCE_API_KEY:-}}"
SEC="${2:-${BINANCE_API_SECRET:-}}"

if [[ -z "${KEY}" || -z "${SEC}" ]]; then
  echo "ERROR: Missing key/secret. Provide args or BINANCE_API_KEY/BINANCE_API_SECRET env vars." >&2
  exit 2
fi

touch .env

tmp=".env.tmp.$$"
trap 'rm -f "${tmp}"' EXIT

# Strip existing sensitive lines
grep -v '^BINANCE_API_KEY=' .env 2>/dev/null | \
  grep -v '^BINANCE_API_SECRET=' > "${tmp}" || true

# Ensure baseline flags exist
if ! grep -q '^ENV=' "${tmp}" 2>/dev/null; then
  echo 'ENV=prod' >> "${tmp}"
fi
if ! grep -q '^BINANCE_TESTNET=' "${tmp}" 2>/dev/null; then
  echo 'BINANCE_TESTNET=0' >> "${tmp}"
fi
if ! grep -q '^DRY_RUN=' "${tmp}" 2>/dev/null; then
  echo 'DRY_RUN=1' >> "${tmp}"
fi

{
  echo "BINANCE_API_KEY=${KEY}"
  echo "BINANCE_API_SECRET=${SEC}"
} >> "${tmp}"

mv "${tmp}" .env
trap - EXIT
rm -f "${tmp}" 2>/dev/null || true
chmod 600 .env

printf '[ok] .env updated (key_len=%s, secret_len=%s)\n' "${#KEY}" "${#SEC}"
echo "NOTE: Secrets not printed. To use now in this shell: run 'set -a; source ./.env; set +a'"

## utils/__init__.py

## utils/firestore_client.py
from functools import lru_cache
import os
import sys
import json
import base64
from typing import Any, Dict, TYPE_CHECKING

service_account: Any
firestore: Any
try:
    from google.oauth2 import service_account as _service_account
    from google.cloud import firestore as _firestore
except Exception:  # pragma: no cover - optional dependency
    service_account = None
    firestore = None
else:
    service_account = _service_account
    firestore = _firestore

if TYPE_CHECKING:  # pragma: no cover - type hint only
    from google.cloud.firestore import Client as FirestoreClient
else:
    FirestoreClient = Any

_ADC_WARNED = False

REQUIRED_FIELDS = ("project_id", "client_email", "private_key")


def _load_creds_dict() -> Dict[str, Any]:
    """Load Firestore service-account JSON via multiple fallbacks:
    1) FIREBASE_CREDS_PATH -> file path
    2) FIREBASE_CREDS_JSON -> raw JSON or base64-encoded JSON
    3) GOOGLE_APPLICATION_CREDENTIALS -> file path (GCP standard)
    4) ./config/firebase_creds.json -> repo default
    """
    # 1) Explicit path
    path = os.environ.get("FIREBASE_CREDS_PATH")
    if path and os.path.exists(path):
        os.environ.setdefault("GOOGLE_APPLICATION_CREDENTIALS", path)
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)

    # 2) Inline env JSON (raw or base64)
    inline = os.environ.get("FIREBASE_CREDS_JSON")
    if inline:
        try:
            # Try raw JSON first
            return json.loads(inline)
        except Exception:
            try:
                decoded = base64.b64decode(inline).decode("utf-8")
                return json.loads(decoded)
            except Exception as e:
                raise RuntimeError(f"FIREBASE_CREDS_JSON invalid (not JSON or base64): {e}")

    # 3) Standard GCP path
    gpath = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS")
    if gpath and os.path.exists(gpath):
        with open(gpath, "r", encoding="utf-8") as f:
            return json.load(f)

    # 4) Repo default
    default_path = os.path.join(os.getcwd(), "config", "firebase_creds.json")
    if os.path.exists(default_path):
        with open(default_path, "r", encoding="utf-8") as f:
            return json.load(f)

    raise RuntimeError(
        "Firebase credentials not found. Set FIREBASE_CREDS_PATH (file), "
        "FIREBASE_CREDS_JSON (json/base64), or place config/firebase_creds.json."
    )


def _validate_creds(info: Dict[str, Any]) -> None:
    missing = [k for k in REQUIRED_FIELDS if not info.get(k)]
    if missing:
        raise RuntimeError(f"Firebase creds missing fields: {', '.join(missing)}")


def _noop_db():
    class _NoopDoc:
        def set(self, *_a, **_k):
            return None

        def update(self, *_a, **_k):
            return None

        def get(self):
            class _Empty:
                def to_dict(self_inner):
                    return {}

            return _Empty()

    class _NoopCollection:
        def document(self, *_a, **_k):
            return _NoopDoc()

        def add(self, *_a, **_k):
            return (None, None)

        def where(self, *_a, **_k):
            return self

    class _NoopDB:
        def collection(self, *_a, **_k):
            return _NoopCollection()

    return _NoopDB()


@lru_cache(maxsize=1)
def get_db() -> FirestoreClient:
    global _ADC_WARNED
    if os.environ.get("FIRESTORE_ENABLED", "1") == "0":
        return _noop_db()
    if firestore is None or service_account is None:
        if not _ADC_WARNED:
            _ADC_WARNED = True
            print(
                "[firestore] WARN client unavailable; install google-cloud-firestore",
                file=sys.stderr,
            )
        return _noop_db()
    try:
        info = _load_creds_dict()
        _validate_creds(info)

        project_id = os.environ.get("FIREBASE_PROJECT_ID") or info.get("project_id")
        creds = service_account.Credentials.from_service_account_info(info)
        return firestore.Client(credentials=creds, project=project_id)
    except Exception as exc:
        fallback_exc = exc
        # Fallback: use GOOGLE_APPLICATION_CREDENTIALS directly if valid
        gac = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS")
        if gac and os.path.exists(gac):
            try:
                return firestore.Client()
            except Exception as inner:
                fallback_exc = inner
        if not _ADC_WARNED:
            _ADC_WARNED = True
            print(
                f"[firestore] WARN ADC unavailable; running in no-op mode: {fallback_exc}",
                file=sys.stderr,
            )
        return _noop_db()
